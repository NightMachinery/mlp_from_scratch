{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ooJtNr5dGrH9"
   },
   "source": [
    "**Name:** Feraidoon Mehri\n",
    "\n",
    "**Student Number:** 401205507\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJm9Z1k0cdmh"
   },
   "source": [
    "# Neural-Network with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDN075MYGesD"
   },
   "source": [
    "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3FdxgNcdmm"
   },
   "source": [
    "## Imports & Seeding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZ4zlnxqhl5"
   },
   "source": [
    "Importing some common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Et7OS7TGcdmn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "#: jax used for testing our implementation\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa2v2-xbcdmo"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWqV2Gycdmp"
   },
   "source": [
    "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "d = fetch_openml(data_id=40996)\n",
    "#: https://www.openml.org/search?type=data&status=active&id=40996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d['frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    33.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0  ...     119.0     114.0     130.0      76.0       0.0   \n",
       "2         22.0  ...       0.0       0.0       1.0       0.0       0.0   \n",
       "3         96.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69996     31.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69997      0.0  ...      27.0       0.0       0.0       0.0       0.0   \n",
       "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995       0.0       0.0       0.0       0.0       0.0  \n",
       "69996       0.0       0.0       0.0       0.0       0.0  \n",
       "69997       0.0       0.0       0.0       0.0       0.0  \n",
       "69998       0.0       0.0       0.0       0.0       0.0  \n",
       "69999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9\n",
       "1        0\n",
       "2        0\n",
       "3        3\n",
       "4        0\n",
       "        ..\n",
       "69995    9\n",
       "69996    1\n",
       "69997    8\n",
       "69998    1\n",
       "69999    5\n",
       "Name: class, Length: 70000, dtype: category\n",
       "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_np = d['data'].to_numpy()\n",
    "d_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_np[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np = d['target'].to_numpy().astype(np.int64)\n",
    "y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d_np\n",
    "y = y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tMYZtSoLc7c-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using `fetch_openml`, download `Fashion-MNIST` \n",
    "# and save the training data and labels in `X` and `y` respectively.\n",
    "#############################\n",
    "# Your code goes here (5 points)\n",
    "#: See cells above\n",
    "#############################\n",
    "\n",
    "# Normalization:\n",
    "X = ((X / 255.) - .5) * 2\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Split arrays or matrices into random train and test subsets.\n",
      "\n",
      "Quick utility that wraps input validation and\n",
      "``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "into a single call for splitting (and optionally subsampling) data in a\n",
      "oneliner.\n",
      "\n",
      "Read more in the :ref:`User Guide <cross_validation>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "*arrays : sequence of indexables with same length / shape[0]\n",
      "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "    matrices or pandas dataframes.\n",
      "\n",
      "test_size : float or int, default=None\n",
      "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "    of the dataset to include in the test split. If int, represents the\n",
      "    absolute number of test samples. If None, the value is set to the\n",
      "    complement of the train size. If ``train_size`` is also None, it will\n",
      "    be set to 0.25.\n",
      "\n",
      "train_size : float or int, default=None\n",
      "    If float, should be between 0.0 and 1.0 and represent the\n",
      "    proportion of the dataset to include in the train split. If\n",
      "    int, represents the absolute number of train samples. If None,\n",
      "    the value is automatically set to the complement of the test size.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the shuffling applied to the data before applying the split.\n",
      "    Pass an int for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "shuffle : bool, default=True\n",
      "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "    then stratify must be None.\n",
      "\n",
      "stratify : array-like, default=None\n",
      "    If not None, data is split in a stratified fashion, using this as\n",
      "    the class labels.\n",
      "    Read more in the :ref:`User Guide <stratification>`.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "splitting : list, length=2 * len(arrays)\n",
      "    List containing train-test split of inputs.\n",
      "\n",
      "    .. versionadded:: 0.16\n",
      "        If the input is sparse, the output will be a\n",
      "        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "        input type.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.model_selection import train_test_split\n",
      ">>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      ">>> X\n",
      "array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7],\n",
      "       [8, 9]])\n",
      ">>> list(y)\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "...     X, y, test_size=0.33, random_state=42)\n",
      "...\n",
      ">>> X_train\n",
      "array([[4, 5],\n",
      "       [0, 1],\n",
      "       [6, 7]])\n",
      ">>> y_train\n",
      "[2, 0, 3]\n",
      ">>> X_test\n",
      "array([[2, 3],\n",
      "       [8, 9]])\n",
      ">>> y_test\n",
      "[1, 4]\n",
      "\n",
      ">>> train_test_split(y, shuffle=False)\n",
      "[[0, 1, 2], [3, 4]]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/lib/python3.10/site-packages/sklearn/model_selection/_split.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sDmxyMJ4dBk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using `train_test_split`, split your data into two sets. \n",
    "# Set the test_size to 10000\n",
    "\n",
    "#############################\n",
    "# Your code goes here (6 points)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True, test_size=10000, random_state=42)\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiGTXGXKcdmt"
   },
   "source": [
    "## Prepare training & validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3nNYlDcdmt"
   },
   "source": [
    "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
    "\n",
    "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
    "\n",
    "In this part, you'll limit the testing and training sets to only these three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Calculates ``element in test_elements``, broadcasting over `element` only.\n",
      "Returns a boolean array of the same shape as `element` that is True\n",
      "where an element of `element` is in `test_elements` and False otherwise.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "element : array_like\n",
      "    Input array.\n",
      "test_elements : array_like\n",
      "    The values against which to test each value of `element`.\n",
      "    This argument is flattened if it is an array or array_like.\n",
      "    See notes for behavior with non-array-like parameters.\n",
      "assume_unique : bool, optional\n",
      "    If True, the input arrays are both assumed to be unique, which\n",
      "    can speed up the calculation.  Default is False.\n",
      "invert : bool, optional\n",
      "    If True, the values in the returned array are inverted, as if\n",
      "    calculating `element not in test_elements`. Default is False.\n",
      "    ``np.isin(a, b, invert=True)`` is equivalent to (but faster\n",
      "    than) ``np.invert(np.isin(a, b))``.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "isin : ndarray, bool\n",
      "    Has the same shape as `element`. The values `element[isin]`\n",
      "    are in `test_elements`.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "in1d                  : Flattened version of this function.\n",
      "numpy.lib.arraysetops : Module with a number of other functions for\n",
      "                        performing set operations on arrays.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "`isin` is an element-wise function version of the python keyword `in`.\n",
      "``isin(a, b)`` is roughly equivalent to\n",
      "``np.array([item in b for item in a])`` if `a` and `b` are 1-D sequences.\n",
      "\n",
      "`element` and `test_elements` are converted to arrays if they are not\n",
      "already. If `test_elements` is a set (or other non-sequence collection)\n",
      "it will be converted to an object array with one element, rather than an\n",
      "array of the values contained in `test_elements`. This is a consequence\n",
      "of the `array` constructor's way of handling non-sequence collections.\n",
      "Converting the set to a list usually gives the desired behavior.\n",
      "\n",
      ".. versionadded:: 1.13.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> element = 2*np.arange(4).reshape((2, 2))\n",
      ">>> element\n",
      "array([[0, 2],\n",
      "       [4, 6]])\n",
      ">>> test_elements = [1, 2, 4, 8]\n",
      ">>> mask = np.isin(element, test_elements)\n",
      ">>> mask\n",
      "array([[False,  True],\n",
      "       [ True, False]])\n",
      ">>> element[mask]\n",
      "array([2, 4])\n",
      "\n",
      "The indices of the matched values can be obtained with `nonzero`:\n",
      "\n",
      ">>> np.nonzero(mask)\n",
      "(array([0, 1]), array([1, 0]))\n",
      "\n",
      "The test can also be inverted:\n",
      "\n",
      ">>> mask = np.isin(element, test_elements, invert=True)\n",
      ">>> mask\n",
      "array([[ True, False],\n",
      "       [False,  True]])\n",
      ">>> element[mask]\n",
      "array([0, 6])\n",
      "\n",
      "Because of how `array` handles sets, the following does not\n",
      "work as expected:\n",
      "\n",
      ">>> test_set = {1, 2, 4, 8}\n",
      ">>> np.isin(element, test_set)\n",
      "array([[False, False],\n",
      "       [False, False]])\n",
      "\n",
      "Casting the set to a list gives the expected result:\n",
      "\n",
      ">>> np.isin(element, list(test_set))\n",
      "array([[False,  True],\n",
      "       [ True, False]])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/lib/python3.10/site-packages/numpy/lib/arraysetops.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "np.isin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TcBDZEtzcdmu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 784) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_train` and `x_train`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "# get indices of y that have values 0, 1, or 7:\n",
    "idx = np.isin(y_train, np.array([0, 1, 7]))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 7, 1, ..., 7, 7, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LX2hkRe1cdmw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784) (3000,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_test` and `x_test`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "idx = np.isin(y_test, np.array([0, 1, 7]))\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]\n",
    "#############################\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv6SMLUktWbv"
   },
   "source": [
    "## Linear & Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXlyJo5JteKC"
   },
   "source": [
    "In this part, you'll implement the forward and backward process for the following components:\n",
    "- Softmax Layer\n",
    "- Linear Layer\n",
    "- ReLU Layer\n",
    "- Sigmoid Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXtAD5uYA4sQ"
   },
   "source": [
    "### The `Softmax` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "tzaIVo-_Axp7"
   },
   "outputs": [],
   "source": [
    "class SoftMaxLayer(object):\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write the forward pass for softmax.\n",
    "        # Save the values required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        # self.inp = x #: @unnecessary\n",
    "        \n",
    "        m = np.max(x, axis=-1, keepdims=True)\n",
    "        self.output = np.exp(x - m)\n",
    "        self.output /= np.sum(self.output, axis=-1, keepdims=True)\n",
    "        #############################\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, up_grad, return_dict=False):\n",
    "        # Write the backward pass for softmax.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        #: We return the gradient of the loss with respect to the inputs.\n",
    "        #: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        #: output: (B, F)\n",
    "        #: up_grad: (B, F)\n",
    "        #: local_grad: (B, F, F)\n",
    "        t_1 = (np.eye(self.output.shape[-1]) - self.output.reshape(-1, 1, self.output.shape[-1]))\n",
    "        #: t_1: (B, F, F)\n",
    "\n",
    "        t_2 = np.einsum('...i,...ij->...ij', self.output, t_1)\n",
    "        # ic(t_1, t_1.shape, t_2, t_2.shape)\n",
    "        # ic(t_1.shape, t_2.shape)\n",
    "        \n",
    "        # t_2 = np.diag(self.output.reshape(-1))\n",
    "        # local_grad = np.matmul(self.output, ) \n",
    "        # ic(t_2)\n",
    "        res = np.einsum('...ij,...j->...i', t_2, up_grad)\n",
    "        #: res: (B, F)\n",
    "\n",
    "        if return_dict:\n",
    "            return dict(res=res, jac=t_2)\n",
    "        else:\n",
    "            return res\n",
    "        #############################\n",
    "\n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tests for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'float'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "N : int\n",
      "  Number of rows in the output.\n",
      "M : int, optional\n",
      "  Number of columns in the output. If None, defaults to `N`.\n",
      "k : int, optional\n",
      "  Index of the diagonal: 0 (the default) refers to the main diagonal,\n",
      "  a positive value refers to an upper diagonal, and a negative value\n",
      "  to a lower diagonal.\n",
      "dtype : data-type, optional\n",
      "  Data-type of the returned array.\n",
      "order : {'C', 'F'}, optional\n",
      "    Whether the output should be stored in row-major (C-style) or\n",
      "    column-major (Fortran-style) order in memory.\n",
      "\n",
      "    .. versionadded:: 1.14.0\n",
      "like : array_like, optional\n",
      "    Reference object to allow the creation of arrays which are not\n",
      "    NumPy arrays. If an array-like passed in as ``like`` supports\n",
      "    the ``__array_function__`` protocol, the result will be defined\n",
      "    by it. In this case, it ensures the creation of an array object\n",
      "    compatible with that passed in via this argument.\n",
      "\n",
      "    .. versionadded:: 1.20.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "I : ndarray of shape (N,M)\n",
      "  An array where all elements are equal to zero, except for the `k`-th\n",
      "  diagonal, whose values are equal to one.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "identity : (almost) equivalent function\n",
      "diag : diagonal 2-D array from a 1-D array specified by the user.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> np.eye(2, dtype=int)\n",
      "array([[1, 0],\n",
      "       [0, 1]])\n",
      ">>> np.eye(3, k=1)\n",
      "array([[0.,  1.,  0.],\n",
      "       [0.,  0.,  1.],\n",
      "       [0.,  0.,  0.]])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/lib/python3.10/site-packages/numpy/lib/twodim_base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "np.eye?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3),\n",
       " array([[6.96469186, 2.86139335, 2.26851454],\n",
       "        [5.51314769, 7.1946897 , 4.2310646 ],\n",
       "        [9.80764198, 6.84829739, 4.80931901],\n",
       "        [3.92117518, 3.43178016, 7.29049707],\n",
       "        [4.38572245, 0.59677897, 3.98044255],\n",
       "        [7.37995406, 1.8249173 , 1.75451756],\n",
       "        [5.31551374, 5.31827587, 6.34400959],\n",
       "        [8.49431794, 7.24455325, 6.11023511],\n",
       "        [7.22443383, 3.22958914, 3.61788656],\n",
       "        [2.28263231, 2.93714046, 6.30976124]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_x = np.random.random((10, 3))*1e1\n",
    "sm1_x.shape, sm1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sm1.forward(sm1_x): array([[0.97499317, 0.01610504, 0.0089018 ],\n",
      "                               [0.15034667, 0.80793834, 0.04171499],\n",
      "                               [0.94464195, 0.04898242, 0.00637563],\n",
      "                               [0.03260322, 0.01998568, 0.94741109],\n",
      "                               [0.59192275, 0.01338899, 0.39468826],\n",
      "                               [0.99258251, 0.00383924, 0.00357825],\n",
      "                               [0.20834975, 0.20892604, 0.58272421],\n",
      "                               [0.72529691, 0.20784995, 0.06685314],\n",
      "                               [0.95642918, 0.01760815, 0.02596266],\n",
      "                               [0.01694232, 0.03260033, 0.95045735]])\n",
      "ic| t_1.shape: (10, 3, 3), t_2.shape: (10, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, 3),\n",
       " array([[-2.42861287e-17,  3.46944695e-18,  2.43945489e-19],\n",
       "        [-2.77555756e-17, -1.11022302e-16, -1.38777878e-17],\n",
       "        [-9.02056208e-17, -6.93889390e-18, -5.96311195e-19],\n",
       "        [-2.71050543e-18,  0.00000000e+00, -2.42861287e-17],\n",
       "        [ 5.20417043e-17,  1.73472348e-18,  4.77048956e-17],\n",
       "        [-4.46691295e-17, -4.33680869e-19, -2.91379334e-19],\n",
       "        [-1.38777878e-17,  0.00000000e+00,  1.38777878e-17],\n",
       "        [-8.32667268e-17,  0.00000000e+00, -6.93889390e-18],\n",
       "        [ 1.38777878e-17,  3.46944695e-18, -6.50521303e-19],\n",
       "        [-1.95156391e-18,  0.00000000e+00, -1.11022302e-16]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1 = SoftMaxLayer()\n",
    "\n",
    "ic(sm1.forward(sm1_x))\n",
    "\n",
    "sm1_g_dict = sm1.backward(np.ones_like(sm1_x), return_dict=True)\n",
    "sm1_g = sm1_g_dict['res']\n",
    "sm1_jac = sm1_g_dict['jac']\n",
    "sm1_g.shape, sm1_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# from jax import softmax:\n",
    "# https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html#jax.nn.softmax\n",
    "def jsoftmax(x):\n",
    "    x = x - jnp.max(x, axis=-1, keepdims=True)\n",
    "    x = jnp.exp(x)\n",
    "    x = x / jnp.sum(x, axis=-1, keepdims=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.97499317, 0.01610504, 0.0089018 ],\n",
       "             [0.15034667, 0.8079383 , 0.04171499],\n",
       "             [0.9446419 , 0.04898242, 0.00637563],\n",
       "             [0.03260322, 0.01998568, 0.94741106],\n",
       "             [0.5919228 , 0.01338899, 0.39468822],\n",
       "             [0.99258244, 0.00383924, 0.00357825],\n",
       "             [0.20834973, 0.20892607, 0.58272415],\n",
       "             [0.7252969 , 0.2078499 , 0.06685314],\n",
       "             [0.9564292 , 0.01760815, 0.02596266],\n",
       "             [0.01694232, 0.03260034, 0.95045733]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_j_x = jnp.array(sm1_x)\n",
    "sm1_j_y = jsoftmax(sm1_j_x)\n",
    "sm1_j_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 2.43814960e-02, -1.57023035e-02, -8.67919344e-03],\n",
       "              [-1.57023035e-02,  1.58456676e-02, -1.43363839e-04],\n",
       "              [-8.67919344e-03, -1.43363824e-04,  8.82255752e-03]],\n",
       "\n",
       "             [[ 1.27742559e-01, -1.21470846e-01, -6.27171062e-03],\n",
       "              [-1.21470831e-01,  1.55173972e-01, -3.37031409e-02],\n",
       "              [-6.27171062e-03, -3.37031409e-02,  3.99748534e-02]],\n",
       "\n",
       "             [[ 5.22935390e-02, -4.62708510e-02, -6.02268800e-03],\n",
       "              [-4.62708510e-02,  4.65831459e-02, -3.12293851e-04],\n",
       "              [-6.02268800e-03, -3.12293851e-04,  6.33498188e-03]],\n",
       "\n",
       "             [[ 3.15402448e-02, -6.51597453e-04, -3.08886468e-02],\n",
       "              [-6.51597453e-04,  1.95862502e-02, -1.89346522e-02],\n",
       "              [-3.08886524e-02, -1.89346541e-02,  4.98233065e-02]],\n",
       "\n",
       "             [[ 2.41550177e-01, -7.92524591e-03, -2.33624935e-01],\n",
       "              [-7.92524777e-03,  1.32097220e-02, -5.28447470e-03],\n",
       "              [-2.33624980e-01, -5.28447516e-03,  2.38909453e-01]],\n",
       "\n",
       "             [[ 7.36246724e-03, -3.81075894e-03, -3.55170807e-03],\n",
       "              [-3.81075894e-03,  3.82449664e-03, -1.37377474e-05],\n",
       "              [-3.55170807e-03, -1.37377474e-05,  3.56544578e-03]],\n",
       "\n",
       "             [[ 1.64940119e-01, -4.35296930e-02, -1.21410429e-01],\n",
       "              [-4.35296893e-02,  1.65275961e-01, -1.21746272e-01],\n",
       "              [-1.21410429e-01, -1.21746264e-01,  2.43156701e-01]],\n",
       "\n",
       "             [[ 1.99241281e-01, -1.50752902e-01, -4.84883823e-02],\n",
       "              [-1.50752902e-01,  1.64648324e-01, -1.38954204e-02],\n",
       "              [-4.84883860e-02, -1.38954204e-02,  6.23838045e-02]],\n",
       "\n",
       "             [[ 4.16724011e-02, -1.68409534e-02, -2.48314459e-02],\n",
       "              [-1.68409534e-02,  1.72981080e-02, -4.57154558e-04],\n",
       "              [-2.48314459e-02, -4.57154529e-04,  2.52886005e-02]],\n",
       "\n",
       "             [[ 1.66552775e-02, -5.52325335e-04, -1.61029529e-02],\n",
       "              [-5.52325335e-04,  3.15375552e-02, -3.09852306e-02],\n",
       "              [-1.61029510e-02, -3.09852306e-02,  4.70881835e-02]]],            dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_j_jac = jax.vmap(jax.jacobian(jsoftmax))(sm1_j_x)\n",
    "sm1_j_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_jac.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_j_jac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(sm1_jac, np.asarray(sm1_j_jac), atol=1e-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-9.3132257e-10,  2.3283064e-10,  0.0000000e+00],\n",
       "             [ 1.8626451e-09,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  9.6042641e-10,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 0.0000000e+00, -4.6566129e-10,  0.0000000e+00],\n",
       "             [ 2.3283064e-10, -4.0927262e-11,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [-3.7252903e-09,  1.8626451e-09, -3.7252903e-09],\n",
       "             [ 1.8626451e-09,  2.9103830e-11,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.einsum('...ij,...j->...i', sm1_j_jac, jnp.ones_like(sm1_j_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-9.3132257e-10,  2.3283064e-10,  0.0000000e+00],\n",
       "             [ 1.8626451e-09,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  9.6042641e-10,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 0.0000000e+00, -4.6566129e-10,  0.0000000e+00],\n",
       "             [ 2.3283064e-10, -4.0927262e-11,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [-3.7252903e-09,  1.8626451e-09, -3.7252903e-09],\n",
       "             [ 1.8626451e-09,  2.9103830e-11,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a(a, b):\n",
    "    return jnp.dot(a, b)\n",
    "\n",
    "jax.vmap(a, in_axes=0)(sm1_j_jac, jnp.ones_like(sm1_j_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.0000000e+00, -1.5425030e-09,  0.0000000e+00],\n",
       "             [-5.1222742e-09, -1.1175871e-08,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  1.0186341e-09,  0.0000000e+00],\n",
       "             [ 1.8626451e-09,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [ 2.3283064e-10, -3.7289283e-11,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "             [-1.1175871e-08,  9.3132257e-10,  0.0000000e+00],\n",
       "             [ 0.0000000e+00,  2.9103830e-11,  0.0000000e+00],\n",
       "             [ 1.8626451e-09,  0.0000000e+00,  0.0000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.einsum('...ij,...j->...i', sm1_jac, np.ones_like(sm1_j_x, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones_like(sm1_j_x, dtype=np.float64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_jac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.42861287e-17,  3.46944695e-18,  2.43945489e-19],\n",
       "       [-2.77555756e-17, -1.11022302e-16, -1.38777878e-17],\n",
       "       [-9.02056208e-17, -6.93889390e-18, -5.96311195e-19],\n",
       "       [-2.71050543e-18,  0.00000000e+00, -2.42861287e-17],\n",
       "       [ 5.20417043e-17,  1.73472348e-18,  4.77048956e-17],\n",
       "       [-4.46691295e-17, -4.33680869e-19, -2.91379334e-19],\n",
       "       [-1.38777878e-17,  0.00000000e+00,  1.38777878e-17],\n",
       "       [-8.32667268e-17,  0.00000000e+00, -6.93889390e-18],\n",
       "       [ 1.38777878e-17,  3.46944695e-18, -6.50521303e-19],\n",
       "       [-1.95156391e-18,  0.00000000e+00, -1.11022302e-16]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('...ij,...j->...i', sm1_jac, np.ones_like(sm1_j_x, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.42861287e-17,  3.46944695e-18,  2.43945489e-19],\n",
       "       [-2.77555756e-17, -1.11022302e-16, -1.38777878e-17],\n",
       "       [-9.02056208e-17, -6.93889390e-18, -5.96311195e-19],\n",
       "       [-2.71050543e-18,  0.00000000e+00, -2.42861287e-17],\n",
       "       [ 5.20417043e-17,  1.73472348e-18,  4.77048956e-17],\n",
       "       [-4.46691295e-17, -4.33680869e-19, -2.91379334e-19],\n",
       "       [-1.38777878e-17,  0.00000000e+00,  1.38777878e-17],\n",
       "       [-8.32667268e-17,  0.00000000e+00, -6.93889390e-18],\n",
       "       [ 1.38777878e-17,  3.46944695e-18, -6.50521303e-19],\n",
       "       [-1.95156391e-18,  0.00000000e+00, -1.11022302e-16]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcFoIDZjcdnB"
   },
   "source": [
    "### The `Linear` Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "1strsTh6cdnG"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, seed=43):\n",
    "        # Initialize the layer's weights and biases\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        initer = jax.nn.initializers.glorot_normal()\n",
    "        jk1 = jax.random.PRNGKey(seed)\n",
    "        jk1, jk2 = jax.random.split(jk1, 2)\n",
    "        self.w = np.asarray(initer(jk1, (in_dim, out_dim)))\n",
    "        self.b = np.asarray(initer(jk2, (1, out_dim,))).squeeze()\n",
    "        #############################\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.inp = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # Compute linear layer's output.\n",
    "        # Save the value(s) required for the backward phase.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        #: inp: (B, F_in)\n",
    "        #: w: (F_in, F_out)\n",
    "        z = np.dot(inp, self.w) + self.b\n",
    "        self.inp = inp\n",
    "        #############################\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        # Calculate the gradient with respect to the weights \n",
    "        # and biases and save the results.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        #: up_grad: (B, F_out)\n",
    "        inp = self.inp\n",
    "\n",
    "        down_grad = np.einsum('...i,...j->...ij', inp, up_grad) #: (B, F_in, F_out)\n",
    "        down_grad_b = up_grad #: (B, F_out)\n",
    "\n",
    "        self.dw = down_grad\n",
    "        self.db = down_grad_b\n",
    "        #############################\n",
    "        return down_grad, down_grad_b\n",
    "    \n",
    "    def backward_input(self, up_grad):\n",
    "        #: With respect to the input x_i, the gradient is: w[row=i] . up_grad\n",
    "        #: up_grad: (B, F_out)\n",
    "        w = self.w\n",
    "\n",
    "        down_grad = np.einsum('iz,...z->...i', w, up_grad) #: (B, F_in)\n",
    "\n",
    "        #############################\n",
    "        return down_grad\n",
    "        \n",
    "    def step(self, optimizer):\n",
    "        # Update the layer's weights and biases\n",
    "        # Update previous_w_update and previous_b_update accordingly\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "        # print(f\"w: {self.w.shape}, dw: {self.dw.shape}\", flush=True)\n",
    "        w_new = optimizer.get_next_update(self.w, self.dw)\n",
    "        # print(f\"w_new: {w_new.shape}\", flush=True)\n",
    "        self.previous_w_update = w_new - self.w\n",
    "        self.w = w_new\n",
    "\n",
    "        # print(f\"b: {self.b.shape}, db: {self.db.shape}\", flush=True)\n",
    "        b_new = optimizer.get_next_update(self.b, self.db)\n",
    "        # print(f\"b_new: {b_new.shape}\", flush=True)\n",
    "        self.previous_b_update = b_new - self.b\n",
    "        self.b = b_new\n",
    "        #############################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initer = jax.nn.initializers.glorot_normal()\n",
    "jk1 = jax.random.PRNGKey(99)\n",
    "jk1, jk2 = jax.random.split(jk1, 2)\n",
    "# ic(np.asarray(initer(jk1, (10, 23))))\n",
    "(np.asarray(initer(jk2, (1, 23,)).squeeze().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Lfo-nhcdnG"
   },
   "source": [
    "### The `ReLU` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "tN6vcirMcdnH"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Write the forward pass for ReLU.\n",
    "        # Save the value(s) required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp\n",
    "\n",
    "        output = np.maximum(np.array([0]), inp)\n",
    "        self.output = output\n",
    "        #############################\n",
    "        return output\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        inp = self.inp #: (B, F_in)\n",
    "        output = self.output #: (B, F_in)\n",
    "        mask = (inp == output) #: (B, F_in)\n",
    "        down_grad = np.einsum('...i,...i->...i', mask, up_grad)\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relu tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14006872, -0.8617549 , -0.25561937],\n",
       "       [-2.79858911, -1.7715331 , -0.69987723]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_1 = ReLU()\n",
    "relu_1.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_1.backward(np.ones_like(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z00KoSI3cdnJ"
   },
   "source": [
    "### The `sigmoid` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "TTYYeL2lcdnJ"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, inp):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        inp_exp = np.exp(inp)\n",
    "        self.out = inp_exp / (1 + inp_exp)\n",
    "        #############################\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        out = self.out #: (B, F_in)\n",
    "        down_grad = np.einsum('...i,...i->...i', np.multiply(out, (1 - out)), up_grad)\n",
    "        #############################\n",
    "        return down_grad\n",
    "    \n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsigmoid(x):\n",
    "    x_sig = jnn.sigmoid(x)\n",
    "    return x_sig\n",
    "    \n",
    "\n",
    "def jsigmoid_sum(x):\n",
    "    return jnp.sum(jsigmoid(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92746243, -0.17363568,  0.00284592],\n",
       "       [ 0.68822271, -0.87953634,  0.28362732]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.7165602 , 0.45669985, 0.5007115 ],\n",
       "             [0.66557145, 0.29327387, 0.5704353 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.3821316, 0.7499737, 1.0711467], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsigmoid_sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71656018, 0.45669981, 0.50071148],\n",
       "       [0.66557144, 0.29327387, 0.57043529]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sigmoid = Sigmoid()\n",
    "a_sig = my_sigmoid.forward(a)\n",
    "a_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20310169, 0.24812509, 0.24999949],\n",
       "       [0.2225861 , 0.20726431, 0.24503887]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sigmoid.backward(np.ones_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.20310168, 0.24812509, 0.2499995 ],\n",
       "             [0.2225861 , 0.2072643 , 0.24503887]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(jax.grad(jsigmoid_sum))(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zngleGY2cdnK"
   },
   "source": [
    "## `Loss` function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISedT4FvcdnK"
   },
   "source": [
    "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "XQyz4ybycdnL"
   },
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    def __init__(self):\n",
    "      pass\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        self.yhat = pred #: (B, F_out)\n",
    "        self.y = target #: (B, F_out)\n",
    "        m = self.y.shape[0] #: m = B\n",
    "        # Commpute and return the loss \n",
    "        #############################\n",
    "        # Your code goes here (8 points)\n",
    "        loss = -np.sum(np.multiply(self.y, np.log(self.yhat))) / m\n",
    "        return loss\n",
    "        #############################\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        # Derivative of loss_fn with respect to a the predicted label.\n",
    "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        m = self.y.shape[0] #: m = B\n",
    "        local_grad = -np.divide(self.y, self.yhat) / m #: (B, F_out)\n",
    "        grad = local_grad\n",
    "        ####### ######################\n",
    "        return grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jloss(pred, target):\n",
    "    m = target.shape[0]\n",
    "    return -jnp.sum(jnp.multiply(target, jnp.log(pred))) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.343505  , 0.13657838, 0.5199166 ],\n",
       "             [0.42604846, 0.33674848, 0.23720309]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jsoftmax(np.random.randn(2, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.7239014 , 0.1000032 , 0.17609541],\n",
       "             [0.9372215 , 0.02740405, 0.03537438]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b = np.eye(3)[np.random.choice(3, 2)]\n",
    "b = jsoftmax(np.random.randn(2, 3))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.9840829, dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jloss(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840828776359558"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss = CELoss()\n",
    "my_loss.forward(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0536985 , -0.36610186, -0.16934967],\n",
       "       [-1.0999001 , -0.0406892 , -0.0745656 ]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.vmap(jax.grad(jloss))(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.0536985 , -0.36610186, -0.16934967],\n",
       "             [-1.0999001 , -0.0406892 , -0.0745656 ]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(jloss)(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xovZI-70kB9I"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "In this section, you'll implement an optimizer classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "h5ADTi5tkVTS"
   },
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_next_update(self, x, dx):\n",
    "        # Compute the new value for 'x' and return the result\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        #: Average dx extra axes to make its shape the same as x:\n",
    "        # dx = np.mean(dx, axis=tuple(range(dx.ndim - x.ndim)), keepdims=False)\n",
    "        #: We need to multiply the learning rate with the batch size anyways, so we might as well just use the sum here and save ourselves the trouble.\n",
    "        dx = np.sum(dx, axis=tuple(range(dx.ndim - x.ndim)), keepdims=False)\n",
    "\n",
    "        # print(f\"dx: {dx.shape}, x: {x.shape}\", flush=True)\n",
    "        # ic(dx)\n",
    "        \n",
    "        return x - np.multiply(self.lr, dx)\n",
    "        #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxxrEEovYEFi"
   },
   "source": [
    "## The Model\n",
    "Now you'll write the base class for a multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "t8SoZeYRcdnY"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layers: List, loss_fn, optimizer):\n",
    "        self.layers = layers \n",
    "        self.losses  = [] \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Pass `inp` to all the layers sequentially\n",
    "        # and return the result.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        h = inp\n",
    "        # print(f'layer 0 input shape: {h.shape}', flush=True)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer.forward(h)\n",
    "            # print(f'layer {i} output shape: {h.shape}', flush=True)\n",
    "        \n",
    "        return h\n",
    "        #############################\n",
    "        \n",
    "    def loss(self, pred, label):\n",
    "        loss = self.loss_fn.forward(pred, label)\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # Start with loss function's gradient and \n",
    "        # do the backward pass on all the layers.\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "        grad = self.loss_fn.backward() #: (B, F_out)\n",
    "        # print(f\"loss layer grad shape: {grad.shape}\", flush=True)\n",
    "\n",
    "        for i, layer in enumerate(reversed(self.layers)):\n",
    "            if hasattr(layer, 'backward_input'):\n",
    "                layer.backward(grad)\n",
    "                grad = layer.backward_input(grad)\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            \n",
    "            # print(f\"layer {(len(self.layers)-1) - i} grad shape: {grad.shape}\", flush=True)\n",
    "            \n",
    "        return grad\n",
    "        #############################\n",
    "        \n",
    "    def update(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "          layer.step(self.optimizer)\n",
    "        #   print(f\"layer {i} updated\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo0rNwYciueF"
   },
   "source": [
    "The following cell encodes training labels into a one-hot representation with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000,), array([0, 7, 1, ..., 7, 7, 1]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "nhJTulaFJ4vR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onehot_enc(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        if val == 7:\n",
    "            val = 2\n",
    "        ary[i, val] = 1\n",
    "    return ary\n",
    "\n",
    "y_train = onehot_enc(y_train, 3)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = onehot_enc(y_test, 3)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    pred = model.forward(x)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    acc = np.sum(pred == y) / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "TS6S_RUwsRkF"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, x, y, start_epoch=0):\n",
    "    for n in range(start_epoch, start_epoch + epochs):\n",
    "      # First do the forward pass. Next, compute the loss.\n",
    "      # Then do the backward pass and finally, update the parameters.\n",
    "      #############################\n",
    "      # Your code goes here (4 points)\n",
    "      pred = model.forward(x)\n",
    "      print(f\"epoch {n}: pred done\", flush=True)\n",
    "      loss = model.loss(pred, y)\n",
    "      model.backward()\n",
    "      print(f\"epoch {n}: backward done\", flush=True)\n",
    "      model.update()\n",
    "      print(f\"epoch {n}: update done\", flush=True)\n",
    "\n",
    "      print(f\"Loss at {n}: {loss:.6f}\", flush=True)\n",
    "      # print(f\"Loss at {n}: {loss:.3f}\", flush=True)\n",
    "      print(f\"epoch {n} test accuracy: {accuracy(model, x_test, y_test)}\")\n",
    "      #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 784), numpy.ndarray)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: pred done\n",
      "epoch 0: backward done\n",
      "epoch 0: update done\n",
      "Loss at 0: 1.123047\n",
      "epoch 0 test accuracy: 0.5653333333333334\n",
      "epoch 1: pred done\n",
      "epoch 1: backward done\n",
      "epoch 1: update done\n",
      "Loss at 1: 0.928988\n",
      "epoch 1 test accuracy: 0.8463333333333334\n",
      "epoch 2: pred done\n",
      "epoch 2: backward done\n",
      "epoch 2: update done\n",
      "Loss at 2: 0.849718\n",
      "epoch 2 test accuracy: 0.816\n",
      "epoch 3: pred done\n",
      "epoch 3: backward done\n",
      "epoch 3: update done\n",
      "Loss at 3: 0.799174\n",
      "epoch 3 test accuracy: 0.8683333333333333\n",
      "epoch 4: pred done\n",
      "epoch 4: backward done\n",
      "epoch 4: update done\n",
      "Loss at 4: 0.763862\n",
      "epoch 4 test accuracy: 0.8893333333333333\n",
      "epoch 5: pred done\n",
      "epoch 5: backward done\n",
      "epoch 5: update done\n",
      "Loss at 5: 0.689284\n",
      "epoch 5 test accuracy: 0.7496666666666667\n",
      "epoch 6: pred done\n",
      "epoch 6: backward done\n",
      "epoch 6: update done\n",
      "Loss at 6: 0.752457\n",
      "epoch 6 test accuracy: 0.6643333333333333\n",
      "epoch 7: pred done\n",
      "epoch 7: backward done\n",
      "epoch 7: update done\n",
      "Loss at 7: 0.894814\n",
      "epoch 7 test accuracy: 0.6726666666666666\n",
      "epoch 8: pred done\n",
      "epoch 8: backward done\n",
      "epoch 8: update done\n",
      "Loss at 8: 0.850830\n",
      "epoch 8 test accuracy: 0.7523333333333333\n",
      "epoch 9: pred done\n",
      "epoch 9: backward done\n",
      "epoch 9: update done\n",
      "Loss at 9: 0.760418\n",
      "epoch 9 test accuracy: 0.8653333333333333\n",
      "epoch 10: pred done\n",
      "epoch 10: backward done\n",
      "epoch 10: update done\n",
      "Loss at 10: 0.695274\n",
      "epoch 10 test accuracy: 0.9426666666666667\n",
      "epoch 11: pred done\n",
      "epoch 11: backward done\n",
      "epoch 11: update done\n",
      "Loss at 11: 0.653338\n",
      "epoch 11 test accuracy: 0.961\n",
      "epoch 12: pred done\n",
      "epoch 12: backward done\n",
      "epoch 12: update done\n",
      "Loss at 12: 0.597509\n",
      "epoch 12 test accuracy: 0.967\n",
      "epoch 13: pred done\n",
      "epoch 13: backward done\n",
      "epoch 13: update done\n",
      "Loss at 13: 0.589285\n",
      "epoch 13 test accuracy: 0.9693333333333334\n",
      "epoch 14: pred done\n",
      "epoch 14: backward done\n",
      "epoch 14: update done\n",
      "Loss at 14: 0.586140\n",
      "epoch 14 test accuracy: 0.97\n",
      "epoch 15: pred done\n",
      "epoch 15: backward done\n",
      "epoch 15: update done\n",
      "Loss at 15: 0.584194\n",
      "epoch 15 test accuracy: 0.97\n",
      "epoch 16: pred done\n",
      "epoch 16: backward done\n",
      "epoch 16: update done\n",
      "Loss at 16: 0.582800\n",
      "epoch 16 test accuracy: 0.9716666666666667\n",
      "epoch 17: pred done\n",
      "epoch 17: backward done\n",
      "epoch 17: update done\n",
      "Loss at 17: 0.581701\n",
      "epoch 17 test accuracy: 0.9723333333333334\n",
      "epoch 18: pred done\n",
      "epoch 18: backward done\n",
      "epoch 18: update done\n",
      "Loss at 18: 0.580781\n",
      "epoch 18 test accuracy: 0.973\n",
      "epoch 19: pred done\n",
      "epoch 19: backward done\n",
      "epoch 19: update done\n",
      "Loss at 19: 0.579976\n",
      "epoch 19 test accuracy: 0.9736666666666667\n",
      "epoch 20: pred done\n",
      "epoch 20: backward done\n",
      "epoch 20: update done\n",
      "Loss at 20: 0.579253\n",
      "epoch 20 test accuracy: 0.9736666666666667\n",
      "epoch 21: pred done\n",
      "epoch 21: backward done\n",
      "epoch 21: update done\n",
      "Loss at 21: 0.578594\n",
      "epoch 21 test accuracy: 0.9743333333333334\n",
      "epoch 22: pred done\n",
      "epoch 22: backward done\n",
      "epoch 22: update done\n",
      "Loss at 22: 0.577982\n",
      "epoch 22 test accuracy: 0.9746666666666667\n",
      "epoch 23: pred done\n",
      "epoch 23: backward done\n",
      "epoch 23: update done\n",
      "Loss at 23: 0.577405\n",
      "epoch 23 test accuracy: 0.975\n",
      "epoch 24: pred done\n",
      "epoch 24: backward done\n",
      "epoch 24: update done\n",
      "Loss at 24: 0.576858\n",
      "epoch 24 test accuracy: 0.975\n",
      "epoch 25: pred done\n",
      "epoch 25: backward done\n",
      "epoch 25: update done\n",
      "Loss at 25: 0.576337\n",
      "epoch 25 test accuracy: 0.975\n",
      "epoch 26: pred done\n",
      "epoch 26: backward done\n",
      "epoch 26: update done\n",
      "Loss at 26: 0.575836\n",
      "epoch 26 test accuracy: 0.9763333333333334\n",
      "epoch 27: pred done\n",
      "epoch 27: backward done\n",
      "epoch 27: update done\n",
      "Loss at 27: 0.575352\n",
      "epoch 27 test accuracy: 0.977\n",
      "epoch 28: pred done\n",
      "epoch 28: backward done\n",
      "epoch 28: update done\n",
      "Loss at 28: 0.574882\n",
      "epoch 28 test accuracy: 0.9773333333333334\n",
      "epoch 29: pred done\n",
      "epoch 29: backward done\n",
      "epoch 29: update done\n",
      "Loss at 29: 0.574427\n",
      "epoch 29 test accuracy: 0.9776666666666667\n",
      "epoch 30: pred done\n",
      "epoch 30: backward done\n",
      "epoch 30: update done\n",
      "Loss at 30: 0.573985\n",
      "epoch 30 test accuracy: 0.9773333333333334\n",
      "epoch 31: pred done\n",
      "epoch 31: backward done\n",
      "epoch 31: update done\n",
      "Loss at 31: 0.573555\n",
      "epoch 31 test accuracy: 0.978\n",
      "epoch 32: pred done\n",
      "epoch 32: backward done\n",
      "epoch 32: update done\n",
      "Loss at 32: 0.573137\n",
      "epoch 32 test accuracy: 0.978\n",
      "epoch 33: pred done\n",
      "epoch 33: backward done\n",
      "epoch 33: update done\n",
      "Loss at 33: 0.572728\n",
      "epoch 33 test accuracy: 0.9783333333333334\n",
      "epoch 34: pred done\n",
      "epoch 34: backward done\n",
      "epoch 34: update done\n",
      "Loss at 34: 0.572329\n",
      "epoch 34 test accuracy: 0.9793333333333333\n",
      "epoch 35: pred done\n",
      "epoch 35: backward done\n",
      "epoch 35: update done\n",
      "Loss at 35: 0.571943\n",
      "epoch 35 test accuracy: 0.9793333333333333\n",
      "epoch 36: pred done\n",
      "epoch 36: backward done\n",
      "epoch 36: update done\n",
      "Loss at 36: 0.571568\n",
      "epoch 36 test accuracy: 0.9803333333333333\n",
      "epoch 37: pred done\n",
      "epoch 37: backward done\n",
      "epoch 37: update done\n",
      "Loss at 37: 0.571204\n",
      "epoch 37 test accuracy: 0.982\n",
      "epoch 38: pred done\n",
      "epoch 38: backward done\n",
      "epoch 38: update done\n",
      "Loss at 38: 0.570852\n",
      "epoch 38 test accuracy: 0.9823333333333333\n",
      "epoch 39: pred done\n",
      "epoch 39: backward done\n",
      "epoch 39: update done\n",
      "Loss at 39: 0.570511\n",
      "epoch 39 test accuracy: 0.982\n",
      "epoch 40: pred done\n",
      "epoch 40: backward done\n",
      "epoch 40: update done\n",
      "Loss at 40: 0.570184\n",
      "epoch 40 test accuracy: 0.9833333333333333\n",
      "epoch 41: pred done\n",
      "epoch 41: backward done\n",
      "epoch 41: update done\n",
      "Loss at 41: 0.569873\n",
      "epoch 41 test accuracy: 0.984\n",
      "epoch 42: pred done\n",
      "epoch 42: backward done\n",
      "epoch 42: update done\n",
      "Loss at 42: 0.569577\n",
      "epoch 42 test accuracy: 0.9846666666666667\n",
      "epoch 43: pred done\n",
      "epoch 43: backward done\n",
      "epoch 43: update done\n",
      "Loss at 43: 0.569297\n",
      "epoch 43 test accuracy: 0.9846666666666667\n",
      "epoch 44: pred done\n",
      "epoch 44: backward done\n",
      "epoch 44: update done\n",
      "Loss at 44: 0.569032\n",
      "epoch 44 test accuracy: 0.985\n",
      "epoch 45: pred done\n",
      "epoch 45: backward done\n",
      "epoch 45: update done\n",
      "Loss at 45: 0.568778\n",
      "epoch 45 test accuracy: 0.9856666666666667\n",
      "epoch 46: pred done\n",
      "epoch 46: backward done\n",
      "epoch 46: update done\n",
      "Loss at 46: 0.568540\n",
      "epoch 46 test accuracy: 0.9856666666666667\n",
      "epoch 47: pred done\n",
      "epoch 47: backward done\n",
      "epoch 47: update done\n",
      "Loss at 47: 0.568315\n",
      "epoch 47 test accuracy: 0.986\n",
      "epoch 48: pred done\n",
      "epoch 48: backward done\n",
      "epoch 48: update done\n",
      "Loss at 48: 0.568103\n",
      "epoch 48 test accuracy: 0.986\n",
      "epoch 49: pred done\n",
      "epoch 49: backward done\n",
      "epoch 49: update done\n",
      "Loss at 49: 0.567900\n",
      "epoch 49 test accuracy: 0.986\n",
      "epoch 50: pred done\n",
      "epoch 50: backward done\n",
      "epoch 50: update done\n",
      "Loss at 50: 0.567707\n",
      "epoch 50 test accuracy: 0.9866666666666667\n",
      "epoch 51: pred done\n",
      "epoch 51: backward done\n",
      "epoch 51: update done\n",
      "Loss at 51: 0.567524\n",
      "epoch 51 test accuracy: 0.9866666666666667\n",
      "epoch 52: pred done\n",
      "epoch 52: backward done\n",
      "epoch 52: update done\n",
      "Loss at 52: 0.567351\n",
      "epoch 52 test accuracy: 0.987\n",
      "epoch 53: pred done\n",
      "epoch 53: backward done\n",
      "epoch 53: update done\n",
      "Loss at 53: 0.567187\n",
      "epoch 53 test accuracy: 0.987\n",
      "epoch 54: pred done\n",
      "epoch 54: backward done\n",
      "epoch 54: update done\n",
      "Loss at 54: 0.567031\n",
      "epoch 54 test accuracy: 0.987\n",
      "epoch 55: pred done\n",
      "epoch 55: backward done\n",
      "epoch 55: update done\n",
      "Loss at 55: 0.566881\n",
      "epoch 55 test accuracy: 0.987\n",
      "epoch 56: pred done\n",
      "epoch 56: backward done\n",
      "epoch 56: update done\n",
      "Loss at 56: 0.566738\n",
      "epoch 56 test accuracy: 0.988\n",
      "epoch 57: pred done\n",
      "epoch 57: backward done\n",
      "epoch 57: update done\n",
      "Loss at 57: 0.566600\n",
      "epoch 57 test accuracy: 0.988\n",
      "epoch 58: pred done\n",
      "epoch 58: backward done\n",
      "epoch 58: update done\n",
      "Loss at 58: 0.566468\n",
      "epoch 58 test accuracy: 0.9883333333333333\n",
      "epoch 59: pred done\n",
      "epoch 59: backward done\n",
      "epoch 59: update done\n",
      "Loss at 59: 0.566343\n",
      "epoch 59 test accuracy: 0.9883333333333333\n",
      "epoch 60: pred done\n",
      "epoch 60: backward done\n",
      "epoch 60: update done\n",
      "Loss at 60: 0.566221\n",
      "epoch 60 test accuracy: 0.9883333333333333\n",
      "epoch 61: pred done\n",
      "epoch 61: backward done\n",
      "epoch 61: update done\n",
      "Loss at 61: 0.566105\n",
      "epoch 61 test accuracy: 0.9883333333333333\n",
      "epoch 62: pred done\n",
      "epoch 62: backward done\n",
      "epoch 62: update done\n",
      "Loss at 62: 0.565992\n",
      "epoch 62 test accuracy: 0.9883333333333333\n",
      "epoch 63: pred done\n",
      "epoch 63: backward done\n",
      "epoch 63: update done\n",
      "Loss at 63: 0.565884\n",
      "epoch 63 test accuracy: 0.9883333333333333\n",
      "epoch 64: pred done\n",
      "epoch 64: backward done\n",
      "epoch 64: update done\n",
      "Loss at 64: 0.565779\n",
      "epoch 64 test accuracy: 0.9886666666666667\n",
      "epoch 65: pred done\n",
      "epoch 65: backward done\n",
      "epoch 65: update done\n",
      "Loss at 65: 0.565678\n",
      "epoch 65 test accuracy: 0.9886666666666667\n",
      "epoch 66: pred done\n",
      "epoch 66: backward done\n",
      "epoch 66: update done\n",
      "Loss at 66: 0.565580\n",
      "epoch 66 test accuracy: 0.9886666666666667\n",
      "epoch 67: pred done\n",
      "epoch 67: backward done\n",
      "epoch 67: update done\n",
      "Loss at 67: 0.565484\n",
      "epoch 67 test accuracy: 0.989\n",
      "epoch 68: pred done\n",
      "epoch 68: backward done\n",
      "epoch 68: update done\n",
      "Loss at 68: 0.565390\n",
      "epoch 68 test accuracy: 0.989\n",
      "epoch 69: pred done\n",
      "epoch 69: backward done\n",
      "epoch 69: update done\n",
      "Loss at 69: 0.565300\n",
      "epoch 69 test accuracy: 0.989\n",
      "epoch 70: pred done\n",
      "epoch 70: backward done\n",
      "epoch 70: update done\n",
      "Loss at 70: 0.565213\n",
      "epoch 70 test accuracy: 0.989\n",
      "epoch 71: pred done\n",
      "epoch 71: backward done\n",
      "epoch 71: update done\n",
      "Loss at 71: 0.565129\n",
      "epoch 71 test accuracy: 0.989\n",
      "epoch 72: pred done\n",
      "epoch 72: backward done\n",
      "epoch 72: update done\n",
      "Loss at 72: 0.565046\n",
      "epoch 72 test accuracy: 0.989\n",
      "epoch 73: pred done\n",
      "epoch 73: backward done\n",
      "epoch 73: update done\n",
      "Loss at 73: 0.564967\n",
      "epoch 73 test accuracy: 0.989\n",
      "epoch 74: pred done\n",
      "epoch 74: backward done\n",
      "epoch 74: update done\n",
      "Loss at 74: 0.564889\n",
      "epoch 74 test accuracy: 0.989\n",
      "epoch 75: pred done\n",
      "epoch 75: backward done\n",
      "epoch 75: update done\n",
      "Loss at 75: 0.564813\n",
      "epoch 75 test accuracy: 0.9893333333333333\n",
      "epoch 76: pred done\n",
      "epoch 76: backward done\n",
      "epoch 76: update done\n",
      "Loss at 76: 0.564739\n",
      "epoch 76 test accuracy: 0.9893333333333333\n",
      "epoch 77: pred done\n",
      "epoch 77: backward done\n",
      "epoch 77: update done\n",
      "Loss at 77: 0.564667\n",
      "epoch 77 test accuracy: 0.9896666666666667\n",
      "epoch 78: pred done\n",
      "epoch 78: backward done\n",
      "epoch 78: update done\n",
      "Loss at 78: 0.564597\n",
      "epoch 78 test accuracy: 0.9896666666666667\n",
      "epoch 79: pred done\n",
      "epoch 79: backward done\n",
      "epoch 79: update done\n",
      "Loss at 79: 0.564528\n",
      "epoch 79 test accuracy: 0.99\n",
      "epoch 80: pred done\n",
      "epoch 80: backward done\n",
      "epoch 80: update done\n",
      "Loss at 80: 0.564460\n",
      "epoch 80 test accuracy: 0.99\n",
      "epoch 81: pred done\n",
      "epoch 81: backward done\n",
      "epoch 81: update done\n",
      "Loss at 81: 0.564394\n",
      "epoch 81 test accuracy: 0.99\n",
      "epoch 82: pred done\n",
      "epoch 82: backward done\n",
      "epoch 82: update done\n",
      "Loss at 82: 0.564329\n",
      "epoch 82 test accuracy: 0.99\n",
      "epoch 83: pred done\n",
      "epoch 83: backward done\n",
      "epoch 83: update done\n",
      "Loss at 83: 0.564266\n",
      "epoch 83 test accuracy: 0.99\n",
      "epoch 84: pred done\n",
      "epoch 84: backward done\n",
      "epoch 84: update done\n",
      "Loss at 84: 0.564204\n",
      "epoch 84 test accuracy: 0.99\n",
      "epoch 85: pred done\n",
      "epoch 85: backward done\n",
      "epoch 85: update done\n",
      "Loss at 85: 0.564143\n",
      "epoch 85 test accuracy: 0.99\n",
      "epoch 86: pred done\n",
      "epoch 86: backward done\n",
      "epoch 86: update done\n",
      "Loss at 86: 0.564083\n",
      "epoch 86 test accuracy: 0.99\n",
      "epoch 87: pred done\n",
      "epoch 87: backward done\n",
      "epoch 87: update done\n",
      "Loss at 87: 0.564024\n",
      "epoch 87 test accuracy: 0.99\n",
      "epoch 88: pred done\n",
      "epoch 88: backward done\n",
      "epoch 88: update done\n",
      "Loss at 88: 0.563966\n",
      "epoch 88 test accuracy: 0.99\n",
      "epoch 89: pred done\n",
      "epoch 89: backward done\n",
      "epoch 89: update done\n",
      "Loss at 89: 0.563910\n",
      "epoch 89 test accuracy: 0.99\n",
      "epoch 90: pred done\n",
      "epoch 90: backward done\n",
      "epoch 90: update done\n",
      "Loss at 90: 0.563854\n",
      "epoch 90 test accuracy: 0.99\n",
      "epoch 91: pred done\n",
      "epoch 91: backward done\n",
      "epoch 91: update done\n",
      "Loss at 91: 0.563799\n",
      "epoch 91 test accuracy: 0.99\n",
      "epoch 92: pred done\n",
      "epoch 92: backward done\n",
      "epoch 92: update done\n",
      "Loss at 92: 0.563745\n",
      "epoch 92 test accuracy: 0.99\n",
      "epoch 93: pred done\n",
      "epoch 93: backward done\n",
      "epoch 93: update done\n",
      "Loss at 93: 0.563692\n",
      "epoch 93 test accuracy: 0.99\n",
      "epoch 94: pred done\n",
      "epoch 94: backward done\n",
      "epoch 94: update done\n",
      "Loss at 94: 0.563641\n",
      "epoch 94 test accuracy: 0.99\n",
      "epoch 95: pred done\n",
      "epoch 95: backward done\n",
      "epoch 95: update done\n",
      "Loss at 95: 0.563590\n",
      "epoch 95 test accuracy: 0.99\n",
      "epoch 96: pred done\n",
      "epoch 96: backward done\n",
      "epoch 96: update done\n",
      "Loss at 96: 0.563539\n",
      "epoch 96 test accuracy: 0.99\n",
      "epoch 97: pred done\n",
      "epoch 97: backward done\n",
      "epoch 97: update done\n",
      "Loss at 97: 0.563490\n",
      "epoch 97 test accuracy: 0.99\n",
      "epoch 98: pred done\n",
      "epoch 98: backward done\n",
      "epoch 98: update done\n",
      "Loss at 98: 0.563441\n",
      "epoch 98 test accuracy: 0.99\n",
      "epoch 99: pred done\n",
      "epoch 99: backward done\n",
      "epoch 99: update done\n",
      "Loss at 99: 0.563393\n",
      "epoch 99 test accuracy: 0.99\n",
      "epoch 100: pred done\n",
      "epoch 100: backward done\n",
      "epoch 100: update done\n",
      "Loss at 100: 0.563346\n",
      "epoch 100 test accuracy: 0.99\n",
      "epoch 101: pred done\n",
      "epoch 101: backward done\n",
      "epoch 101: update done\n",
      "Loss at 101: 0.563299\n",
      "epoch 101 test accuracy: 0.99\n",
      "epoch 102: pred done\n",
      "epoch 102: backward done\n",
      "epoch 102: update done\n",
      "Loss at 102: 0.563252\n",
      "epoch 102 test accuracy: 0.99\n",
      "epoch 103: pred done\n",
      "epoch 103: backward done\n",
      "epoch 103: update done\n",
      "Loss at 103: 0.563207\n",
      "epoch 103 test accuracy: 0.99\n",
      "epoch 104: pred done\n",
      "epoch 104: backward done\n",
      "epoch 104: update done\n",
      "Loss at 104: 0.563162\n",
      "epoch 104 test accuracy: 0.99\n",
      "epoch 105: pred done\n",
      "epoch 105: backward done\n",
      "epoch 105: update done\n",
      "Loss at 105: 0.563118\n",
      "epoch 105 test accuracy: 0.99\n",
      "epoch 106: pred done\n",
      "epoch 106: backward done\n",
      "epoch 106: update done\n",
      "Loss at 106: 0.563075\n",
      "epoch 106 test accuracy: 0.99\n",
      "epoch 107: pred done\n",
      "epoch 107: backward done\n",
      "epoch 107: update done\n",
      "Loss at 107: 0.563032\n",
      "epoch 107 test accuracy: 0.9903333333333333\n",
      "epoch 108: pred done\n",
      "epoch 108: backward done\n",
      "epoch 108: update done\n",
      "Loss at 108: 0.562990\n",
      "epoch 108 test accuracy: 0.9903333333333333\n",
      "epoch 109: pred done\n",
      "epoch 109: backward done\n",
      "epoch 109: update done\n",
      "Loss at 109: 0.562949\n",
      "epoch 109 test accuracy: 0.9903333333333333\n",
      "epoch 110: pred done\n",
      "epoch 110: backward done\n",
      "epoch 110: update done\n",
      "Loss at 110: 0.562909\n",
      "epoch 110 test accuracy: 0.9903333333333333\n",
      "epoch 111: pred done\n",
      "epoch 111: backward done\n",
      "epoch 111: update done\n",
      "Loss at 111: 0.562869\n",
      "epoch 111 test accuracy: 0.9903333333333333\n",
      "epoch 112: pred done\n",
      "epoch 112: backward done\n",
      "epoch 112: update done\n",
      "Loss at 112: 0.562829\n",
      "epoch 112 test accuracy: 0.9903333333333333\n",
      "epoch 113: pred done\n",
      "epoch 113: backward done\n",
      "epoch 113: update done\n",
      "Loss at 113: 0.562791\n",
      "epoch 113 test accuracy: 0.9903333333333333\n",
      "epoch 114: pred done\n",
      "epoch 114: backward done\n",
      "epoch 114: update done\n",
      "Loss at 114: 0.562752\n",
      "epoch 114 test accuracy: 0.9903333333333333\n",
      "epoch 115: pred done\n",
      "epoch 115: backward done\n",
      "epoch 115: update done\n",
      "Loss at 115: 0.562715\n",
      "epoch 115 test accuracy: 0.9906666666666667\n",
      "epoch 116: pred done\n",
      "epoch 116: backward done\n",
      "epoch 116: update done\n",
      "Loss at 116: 0.562678\n",
      "epoch 116 test accuracy: 0.9906666666666667\n",
      "epoch 117: pred done\n",
      "epoch 117: backward done\n",
      "epoch 117: update done\n",
      "Loss at 117: 0.562641\n",
      "epoch 117 test accuracy: 0.9906666666666667\n",
      "epoch 118: pred done\n",
      "epoch 118: backward done\n",
      "epoch 118: update done\n",
      "Loss at 118: 0.562605\n",
      "epoch 118 test accuracy: 0.9906666666666667\n",
      "epoch 119: pred done\n",
      "epoch 119: backward done\n",
      "epoch 119: update done\n",
      "Loss at 119: 0.562569\n",
      "epoch 119 test accuracy: 0.9906666666666667\n",
      "epoch 120: pred done\n",
      "epoch 120: backward done\n",
      "epoch 120: update done\n",
      "Loss at 120: 0.562534\n",
      "epoch 120 test accuracy: 0.9906666666666667\n",
      "epoch 121: pred done\n",
      "epoch 121: backward done\n",
      "epoch 121: update done\n",
      "Loss at 121: 0.562499\n",
      "epoch 121 test accuracy: 0.991\n",
      "epoch 122: pred done\n",
      "epoch 122: backward done\n",
      "epoch 122: update done\n",
      "Loss at 122: 0.562464\n",
      "epoch 122 test accuracy: 0.991\n",
      "epoch 123: pred done\n",
      "epoch 123: backward done\n",
      "epoch 123: update done\n",
      "Loss at 123: 0.562430\n",
      "epoch 123 test accuracy: 0.991\n",
      "epoch 124: pred done\n",
      "epoch 124: backward done\n",
      "epoch 124: update done\n",
      "Loss at 124: 0.562396\n",
      "epoch 124 test accuracy: 0.991\n",
      "epoch 125: pred done\n",
      "epoch 125: backward done\n",
      "epoch 125: update done\n",
      "Loss at 125: 0.562362\n",
      "epoch 125 test accuracy: 0.991\n",
      "epoch 126: pred done\n",
      "epoch 126: backward done\n",
      "epoch 126: update done\n",
      "Loss at 126: 0.562329\n",
      "epoch 126 test accuracy: 0.991\n",
      "epoch 127: pred done\n",
      "epoch 127: backward done\n",
      "epoch 127: update done\n",
      "Loss at 127: 0.562297\n",
      "epoch 127 test accuracy: 0.9906666666666667\n",
      "epoch 128: pred done\n",
      "epoch 128: backward done\n",
      "epoch 128: update done\n",
      "Loss at 128: 0.562264\n",
      "epoch 128 test accuracy: 0.9906666666666667\n",
      "epoch 129: pred done\n",
      "epoch 129: backward done\n",
      "epoch 129: update done\n",
      "Loss at 129: 0.562232\n",
      "epoch 129 test accuracy: 0.9906666666666667\n",
      "epoch 130: pred done\n",
      "epoch 130: backward done\n",
      "epoch 130: update done\n",
      "Loss at 130: 0.562200\n",
      "epoch 130 test accuracy: 0.9906666666666667\n",
      "epoch 131: pred done\n",
      "epoch 131: backward done\n",
      "epoch 131: update done\n",
      "Loss at 131: 0.562168\n",
      "epoch 131 test accuracy: 0.9906666666666667\n",
      "epoch 132: pred done\n",
      "epoch 132: backward done\n",
      "epoch 132: update done\n",
      "Loss at 132: 0.562137\n",
      "epoch 132 test accuracy: 0.9906666666666667\n",
      "epoch 133: pred done\n",
      "epoch 133: backward done\n",
      "epoch 133: update done\n",
      "Loss at 133: 0.562107\n",
      "epoch 133 test accuracy: 0.9906666666666667\n",
      "epoch 134: pred done\n",
      "epoch 134: backward done\n",
      "epoch 134: update done\n",
      "Loss at 134: 0.562076\n",
      "epoch 134 test accuracy: 0.9906666666666667\n",
      "epoch 135: pred done\n",
      "epoch 135: backward done\n",
      "epoch 135: update done\n",
      "Loss at 135: 0.562046\n",
      "epoch 135 test accuracy: 0.9906666666666667\n",
      "epoch 136: pred done\n",
      "epoch 136: backward done\n",
      "epoch 136: update done\n",
      "Loss at 136: 0.562016\n",
      "epoch 136 test accuracy: 0.9906666666666667\n",
      "epoch 137: pred done\n",
      "epoch 137: backward done\n",
      "epoch 137: update done\n",
      "Loss at 137: 0.561987\n",
      "epoch 137 test accuracy: 0.9906666666666667\n",
      "epoch 138: pred done\n",
      "epoch 138: backward done\n",
      "epoch 138: update done\n",
      "Loss at 138: 0.561958\n",
      "epoch 138 test accuracy: 0.9906666666666667\n",
      "epoch 139: pred done\n",
      "epoch 139: backward done\n",
      "epoch 139: update done\n",
      "Loss at 139: 0.561929\n",
      "epoch 139 test accuracy: 0.9906666666666667\n",
      "epoch 140: pred done\n",
      "epoch 140: backward done\n",
      "epoch 140: update done\n",
      "Loss at 140: 0.561901\n",
      "epoch 140 test accuracy: 0.9906666666666667\n",
      "epoch 141: pred done\n",
      "epoch 141: backward done\n",
      "epoch 141: update done\n",
      "Loss at 141: 0.561873\n",
      "epoch 141 test accuracy: 0.9906666666666667\n",
      "epoch 142: pred done\n",
      "epoch 142: backward done\n",
      "epoch 142: update done\n",
      "Loss at 142: 0.561845\n",
      "epoch 142 test accuracy: 0.9906666666666667\n",
      "epoch 143: pred done\n",
      "epoch 143: backward done\n",
      "epoch 143: update done\n",
      "Loss at 143: 0.561818\n",
      "epoch 143 test accuracy: 0.9906666666666667\n",
      "epoch 144: pred done\n",
      "epoch 144: backward done\n",
      "epoch 144: update done\n",
      "Loss at 144: 0.561791\n",
      "epoch 144 test accuracy: 0.9906666666666667\n",
      "epoch 145: pred done\n",
      "epoch 145: backward done\n",
      "epoch 145: update done\n",
      "Loss at 145: 0.561765\n",
      "epoch 145 test accuracy: 0.9906666666666667\n",
      "epoch 146: pred done\n",
      "epoch 146: backward done\n",
      "epoch 146: update done\n",
      "Loss at 146: 0.561739\n",
      "epoch 146 test accuracy: 0.9906666666666667\n",
      "epoch 147: pred done\n",
      "epoch 147: backward done\n",
      "epoch 147: update done\n",
      "Loss at 147: 0.561713\n",
      "epoch 147 test accuracy: 0.9906666666666667\n",
      "epoch 148: pred done\n",
      "epoch 148: backward done\n",
      "epoch 148: update done\n",
      "Loss at 148: 0.561688\n",
      "epoch 148 test accuracy: 0.9906666666666667\n",
      "epoch 149: pred done\n",
      "epoch 149: backward done\n",
      "epoch 149: update done\n",
      "Loss at 149: 0.561663\n",
      "epoch 149 test accuracy: 0.9906666666666667\n",
      "epoch 150: pred done\n",
      "epoch 150: backward done\n",
      "epoch 150: update done\n",
      "Loss at 150: 0.561638\n",
      "epoch 150 test accuracy: 0.9906666666666667\n",
      "epoch 151: pred done\n",
      "epoch 151: backward done\n",
      "epoch 151: update done\n",
      "Loss at 151: 0.561614\n",
      "epoch 151 test accuracy: 0.9906666666666667\n",
      "epoch 152: pred done\n",
      "epoch 152: backward done\n",
      "epoch 152: update done\n",
      "Loss at 152: 0.561590\n",
      "epoch 152 test accuracy: 0.9906666666666667\n",
      "epoch 153: pred done\n",
      "epoch 153: backward done\n",
      "epoch 153: update done\n",
      "Loss at 153: 0.561566\n",
      "epoch 153 test accuracy: 0.9906666666666667\n",
      "epoch 154: pred done\n",
      "epoch 154: backward done\n",
      "epoch 154: update done\n",
      "Loss at 154: 0.561543\n",
      "epoch 154 test accuracy: 0.9906666666666667\n",
      "epoch 155: pred done\n",
      "epoch 155: backward done\n",
      "epoch 155: update done\n",
      "Loss at 155: 0.561519\n",
      "epoch 155 test accuracy: 0.9906666666666667\n",
      "epoch 156: pred done\n",
      "epoch 156: backward done\n",
      "epoch 156: update done\n",
      "Loss at 156: 0.561496\n",
      "epoch 156 test accuracy: 0.9906666666666667\n",
      "epoch 157: pred done\n",
      "epoch 157: backward done\n",
      "epoch 157: update done\n",
      "Loss at 157: 0.561473\n",
      "epoch 157 test accuracy: 0.9906666666666667\n",
      "epoch 158: pred done\n",
      "epoch 158: backward done\n",
      "epoch 158: update done\n",
      "Loss at 158: 0.561451\n",
      "epoch 158 test accuracy: 0.9906666666666667\n",
      "epoch 159: pred done\n",
      "epoch 159: backward done\n",
      "epoch 159: update done\n",
      "Loss at 159: 0.561428\n",
      "epoch 159 test accuracy: 0.9906666666666667\n",
      "epoch 160: pred done\n",
      "epoch 160: backward done\n",
      "epoch 160: update done\n",
      "Loss at 160: 0.561406\n",
      "epoch 160 test accuracy: 0.9906666666666667\n",
      "epoch 161: pred done\n",
      "epoch 161: backward done\n",
      "epoch 161: update done\n",
      "Loss at 161: 0.561385\n",
      "epoch 161 test accuracy: 0.9906666666666667\n",
      "epoch 162: pred done\n",
      "epoch 162: backward done\n",
      "epoch 162: update done\n",
      "Loss at 162: 0.561363\n",
      "epoch 162 test accuracy: 0.9906666666666667\n",
      "epoch 163: pred done\n",
      "epoch 163: backward done\n",
      "epoch 163: update done\n",
      "Loss at 163: 0.561342\n",
      "epoch 163 test accuracy: 0.9906666666666667\n",
      "epoch 164: pred done\n",
      "epoch 164: backward done\n",
      "epoch 164: update done\n",
      "Loss at 164: 0.561321\n",
      "epoch 164 test accuracy: 0.9906666666666667\n",
      "epoch 165: pred done\n",
      "epoch 165: backward done\n",
      "epoch 165: update done\n",
      "Loss at 165: 0.561300\n",
      "epoch 165 test accuracy: 0.9906666666666667\n",
      "epoch 166: pred done\n",
      "epoch 166: backward done\n",
      "epoch 166: update done\n",
      "Loss at 166: 0.561279\n",
      "epoch 166 test accuracy: 0.9906666666666667\n",
      "epoch 167: pred done\n",
      "epoch 167: backward done\n",
      "epoch 167: update done\n",
      "Loss at 167: 0.561259\n",
      "epoch 167 test accuracy: 0.9906666666666667\n",
      "epoch 168: pred done\n",
      "epoch 168: backward done\n",
      "epoch 168: update done\n",
      "Loss at 168: 0.561239\n",
      "epoch 168 test accuracy: 0.9906666666666667\n",
      "epoch 169: pred done\n",
      "epoch 169: backward done\n",
      "epoch 169: update done\n",
      "Loss at 169: 0.561218\n",
      "epoch 169 test accuracy: 0.9906666666666667\n",
      "epoch 170: pred done\n",
      "epoch 170: backward done\n",
      "epoch 170: update done\n",
      "Loss at 170: 0.561199\n",
      "epoch 170 test accuracy: 0.9906666666666667\n",
      "epoch 171: pred done\n",
      "epoch 171: backward done\n",
      "epoch 171: update done\n",
      "Loss at 171: 0.561179\n",
      "epoch 171 test accuracy: 0.9906666666666667\n",
      "epoch 172: pred done\n",
      "epoch 172: backward done\n",
      "epoch 172: update done\n",
      "Loss at 172: 0.561159\n",
      "epoch 172 test accuracy: 0.9906666666666667\n",
      "epoch 173: pred done\n",
      "epoch 173: backward done\n",
      "epoch 173: update done\n",
      "Loss at 173: 0.561140\n",
      "epoch 173 test accuracy: 0.9906666666666667\n",
      "epoch 174: pred done\n",
      "epoch 174: backward done\n",
      "epoch 174: update done\n",
      "Loss at 174: 0.561121\n",
      "epoch 174 test accuracy: 0.9906666666666667\n",
      "epoch 175: pred done\n",
      "epoch 175: backward done\n",
      "epoch 175: update done\n",
      "Loss at 175: 0.561102\n",
      "epoch 175 test accuracy: 0.9906666666666667\n",
      "epoch 176: pred done\n",
      "epoch 176: backward done\n",
      "epoch 176: update done\n",
      "Loss at 176: 0.561083\n",
      "epoch 176 test accuracy: 0.9906666666666667\n",
      "epoch 177: pred done\n",
      "epoch 177: backward done\n",
      "epoch 177: update done\n",
      "Loss at 177: 0.561064\n",
      "epoch 177 test accuracy: 0.9906666666666667\n",
      "epoch 178: pred done\n",
      "epoch 178: backward done\n",
      "epoch 178: update done\n",
      "Loss at 178: 0.561045\n",
      "epoch 178 test accuracy: 0.9906666666666667\n",
      "epoch 179: pred done\n",
      "epoch 179: backward done\n",
      "epoch 179: update done\n",
      "Loss at 179: 0.561027\n",
      "epoch 179 test accuracy: 0.9906666666666667\n",
      "epoch 180: pred done\n",
      "epoch 180: backward done\n",
      "epoch 180: update done\n",
      "Loss at 180: 0.561009\n",
      "epoch 180 test accuracy: 0.9906666666666667\n",
      "epoch 181: pred done\n",
      "epoch 181: backward done\n",
      "epoch 181: update done\n",
      "Loss at 181: 0.560991\n",
      "epoch 181 test accuracy: 0.991\n",
      "epoch 182: pred done\n",
      "epoch 182: backward done\n",
      "epoch 182: update done\n",
      "Loss at 182: 0.560973\n",
      "epoch 182 test accuracy: 0.991\n",
      "epoch 183: pred done\n",
      "epoch 183: backward done\n",
      "epoch 183: update done\n",
      "Loss at 183: 0.560955\n",
      "epoch 183 test accuracy: 0.991\n",
      "epoch 184: pred done\n",
      "epoch 184: backward done\n",
      "epoch 184: update done\n",
      "Loss at 184: 0.560937\n",
      "epoch 184 test accuracy: 0.991\n",
      "epoch 185: pred done\n",
      "epoch 185: backward done\n",
      "epoch 185: update done\n",
      "Loss at 185: 0.560920\n",
      "epoch 185 test accuracy: 0.991\n",
      "epoch 186: pred done\n",
      "epoch 186: backward done\n",
      "epoch 186: update done\n",
      "Loss at 186: 0.560902\n",
      "epoch 186 test accuracy: 0.991\n",
      "epoch 187: pred done\n",
      "epoch 187: backward done\n",
      "epoch 187: update done\n",
      "Loss at 187: 0.560885\n",
      "epoch 187 test accuracy: 0.991\n",
      "epoch 188: pred done\n",
      "epoch 188: backward done\n",
      "epoch 188: update done\n",
      "Loss at 188: 0.560868\n",
      "epoch 188 test accuracy: 0.991\n",
      "epoch 189: pred done\n",
      "epoch 189: backward done\n",
      "epoch 189: update done\n",
      "Loss at 189: 0.560851\n",
      "epoch 189 test accuracy: 0.991\n",
      "epoch 190: pred done\n",
      "epoch 190: backward done\n",
      "epoch 190: update done\n",
      "Loss at 190: 0.560835\n",
      "epoch 190 test accuracy: 0.991\n",
      "epoch 191: pred done\n",
      "epoch 191: backward done\n",
      "epoch 191: update done\n",
      "Loss at 191: 0.560818\n",
      "epoch 191 test accuracy: 0.991\n",
      "epoch 192: pred done\n",
      "epoch 192: backward done\n",
      "epoch 192: update done\n",
      "Loss at 192: 0.560802\n",
      "epoch 192 test accuracy: 0.991\n",
      "epoch 193: pred done\n",
      "epoch 193: backward done\n",
      "epoch 193: update done\n",
      "Loss at 193: 0.560786\n",
      "epoch 193 test accuracy: 0.991\n",
      "epoch 194: pred done\n",
      "epoch 194: backward done\n",
      "epoch 194: update done\n",
      "Loss at 194: 0.560769\n",
      "epoch 194 test accuracy: 0.991\n",
      "epoch 195: pred done\n",
      "epoch 195: backward done\n",
      "epoch 195: update done\n",
      "Loss at 195: 0.560754\n",
      "epoch 195 test accuracy: 0.991\n",
      "epoch 196: pred done\n",
      "epoch 196: backward done\n",
      "epoch 196: update done\n",
      "Loss at 196: 0.560738\n",
      "epoch 196 test accuracy: 0.991\n",
      "epoch 197: pred done\n",
      "epoch 197: backward done\n",
      "epoch 197: update done\n",
      "Loss at 197: 0.560722\n",
      "epoch 197 test accuracy: 0.991\n",
      "epoch 198: pred done\n",
      "epoch 198: backward done\n",
      "epoch 198: update done\n",
      "Loss at 198: 0.560706\n",
      "epoch 198 test accuracy: 0.991\n",
      "epoch 199: pred done\n",
      "epoch 199: backward done\n",
      "epoch 199: update done\n",
      "Loss at 199: 0.560691\n",
      "epoch 199 test accuracy: 0.991\n",
      "epoch 200: pred done\n",
      "epoch 200: backward done\n",
      "epoch 200: update done\n",
      "Loss at 200: 0.560675\n",
      "epoch 200 test accuracy: 0.991\n",
      "epoch 201: pred done\n",
      "epoch 201: backward done\n",
      "epoch 201: update done\n",
      "Loss at 201: 0.560660\n",
      "epoch 201 test accuracy: 0.991\n",
      "epoch 202: pred done\n",
      "epoch 202: backward done\n",
      "epoch 202: update done\n",
      "Loss at 202: 0.560644\n",
      "epoch 202 test accuracy: 0.991\n",
      "epoch 203: pred done\n",
      "epoch 203: backward done\n",
      "epoch 203: update done\n",
      "Loss at 203: 0.560629\n",
      "epoch 203 test accuracy: 0.991\n",
      "epoch 204: pred done\n",
      "epoch 204: backward done\n",
      "epoch 204: update done\n",
      "Loss at 204: 0.560614\n",
      "epoch 204 test accuracy: 0.991\n",
      "epoch 205: pred done\n",
      "epoch 205: backward done\n",
      "epoch 205: update done\n",
      "Loss at 205: 0.560599\n",
      "epoch 205 test accuracy: 0.991\n",
      "epoch 206: pred done\n",
      "epoch 206: backward done\n",
      "epoch 206: update done\n",
      "Loss at 206: 0.560584\n",
      "epoch 206 test accuracy: 0.991\n",
      "epoch 207: pred done\n",
      "epoch 207: backward done\n",
      "epoch 207: update done\n",
      "Loss at 207: 0.560570\n",
      "epoch 207 test accuracy: 0.991\n",
      "epoch 208: pred done\n",
      "epoch 208: backward done\n",
      "epoch 208: update done\n",
      "Loss at 208: 0.560555\n",
      "epoch 208 test accuracy: 0.991\n",
      "epoch 209: pred done\n",
      "epoch 209: backward done\n",
      "epoch 209: update done\n",
      "Loss at 209: 0.560541\n",
      "epoch 209 test accuracy: 0.991\n",
      "epoch 210: pred done\n",
      "epoch 210: backward done\n",
      "epoch 210: update done\n",
      "Loss at 210: 0.560527\n",
      "epoch 210 test accuracy: 0.991\n",
      "epoch 211: pred done\n",
      "epoch 211: backward done\n",
      "epoch 211: update done\n",
      "Loss at 211: 0.560513\n",
      "epoch 211 test accuracy: 0.991\n",
      "epoch 212: pred done\n",
      "epoch 212: backward done\n",
      "epoch 212: update done\n",
      "Loss at 212: 0.560499\n",
      "epoch 212 test accuracy: 0.991\n",
      "epoch 213: pred done\n",
      "epoch 213: backward done\n",
      "epoch 213: update done\n",
      "Loss at 213: 0.560485\n",
      "epoch 213 test accuracy: 0.991\n",
      "epoch 214: pred done\n",
      "epoch 214: backward done\n",
      "epoch 214: update done\n",
      "Loss at 214: 0.560471\n",
      "epoch 214 test accuracy: 0.991\n",
      "epoch 215: pred done\n",
      "epoch 215: backward done\n",
      "epoch 215: update done\n",
      "Loss at 215: 0.560458\n",
      "epoch 215 test accuracy: 0.991\n",
      "epoch 216: pred done\n",
      "epoch 216: backward done\n",
      "epoch 216: update done\n",
      "Loss at 216: 0.560444\n",
      "epoch 216 test accuracy: 0.9913333333333333\n",
      "epoch 217: pred done\n",
      "epoch 217: backward done\n",
      "epoch 217: update done\n",
      "Loss at 217: 0.560431\n",
      "epoch 217 test accuracy: 0.9913333333333333\n",
      "epoch 218: pred done\n",
      "epoch 218: backward done\n",
      "epoch 218: update done\n",
      "Loss at 218: 0.560417\n",
      "epoch 218 test accuracy: 0.9913333333333333\n",
      "epoch 219: pred done\n",
      "epoch 219: backward done\n",
      "epoch 219: update done\n",
      "Loss at 219: 0.560404\n",
      "epoch 219 test accuracy: 0.9913333333333333\n",
      "epoch 220: pred done\n",
      "epoch 220: backward done\n",
      "epoch 220: update done\n",
      "Loss at 220: 0.560391\n",
      "epoch 220 test accuracy: 0.9913333333333333\n",
      "epoch 221: pred done\n",
      "epoch 221: backward done\n",
      "epoch 221: update done\n",
      "Loss at 221: 0.560378\n",
      "epoch 221 test accuracy: 0.9913333333333333\n",
      "epoch 222: pred done\n",
      "epoch 222: backward done\n",
      "epoch 222: update done\n",
      "Loss at 222: 0.560365\n",
      "epoch 222 test accuracy: 0.9913333333333333\n",
      "epoch 223: pred done\n",
      "epoch 223: backward done\n",
      "epoch 223: update done\n",
      "Loss at 223: 0.560352\n",
      "epoch 223 test accuracy: 0.9913333333333333\n",
      "epoch 224: pred done\n",
      "epoch 224: backward done\n",
      "epoch 224: update done\n",
      "Loss at 224: 0.560340\n",
      "epoch 224 test accuracy: 0.9913333333333333\n",
      "epoch 225: pred done\n",
      "epoch 225: backward done\n",
      "epoch 225: update done\n",
      "Loss at 225: 0.560327\n",
      "epoch 225 test accuracy: 0.9913333333333333\n",
      "epoch 226: pred done\n",
      "epoch 226: backward done\n",
      "epoch 226: update done\n",
      "Loss at 226: 0.560314\n",
      "epoch 226 test accuracy: 0.9913333333333333\n",
      "epoch 227: pred done\n",
      "epoch 227: backward done\n",
      "epoch 227: update done\n",
      "Loss at 227: 0.560302\n",
      "epoch 227 test accuracy: 0.9913333333333333\n",
      "epoch 228: pred done\n",
      "epoch 228: backward done\n",
      "epoch 228: update done\n",
      "Loss at 228: 0.560290\n",
      "epoch 228 test accuracy: 0.9913333333333333\n",
      "epoch 229: pred done\n",
      "epoch 229: backward done\n",
      "epoch 229: update done\n",
      "Loss at 229: 0.560277\n",
      "epoch 229 test accuracy: 0.9913333333333333\n",
      "epoch 230: pred done\n",
      "epoch 230: backward done\n",
      "epoch 230: update done\n",
      "Loss at 230: 0.560265\n",
      "epoch 230 test accuracy: 0.9913333333333333\n",
      "epoch 231: pred done\n",
      "epoch 231: backward done\n",
      "epoch 231: update done\n",
      "Loss at 231: 0.560253\n",
      "epoch 231 test accuracy: 0.9913333333333333\n",
      "epoch 232: pred done\n",
      "epoch 232: backward done\n",
      "epoch 232: update done\n",
      "Loss at 232: 0.560241\n",
      "epoch 232 test accuracy: 0.9913333333333333\n",
      "epoch 233: pred done\n",
      "epoch 233: backward done\n",
      "epoch 233: update done\n",
      "Loss at 233: 0.560229\n",
      "epoch 233 test accuracy: 0.9913333333333333\n",
      "epoch 234: pred done\n",
      "epoch 234: backward done\n",
      "epoch 234: update done\n",
      "Loss at 234: 0.560217\n",
      "epoch 234 test accuracy: 0.9913333333333333\n",
      "epoch 235: pred done\n",
      "epoch 235: backward done\n",
      "epoch 235: update done\n",
      "Loss at 235: 0.560205\n",
      "epoch 235 test accuracy: 0.9913333333333333\n",
      "epoch 236: pred done\n",
      "epoch 236: backward done\n",
      "epoch 236: update done\n",
      "Loss at 236: 0.560194\n",
      "epoch 236 test accuracy: 0.9913333333333333\n",
      "epoch 237: pred done\n",
      "epoch 237: backward done\n",
      "epoch 237: update done\n",
      "Loss at 237: 0.560182\n",
      "epoch 237 test accuracy: 0.9913333333333333\n",
      "epoch 238: pred done\n",
      "epoch 238: backward done\n",
      "epoch 238: update done\n",
      "Loss at 238: 0.560170\n",
      "epoch 238 test accuracy: 0.9913333333333333\n",
      "epoch 239: pred done\n",
      "epoch 239: backward done\n",
      "epoch 239: update done\n",
      "Loss at 239: 0.560159\n",
      "epoch 239 test accuracy: 0.9913333333333333\n",
      "epoch 240: pred done\n",
      "epoch 240: backward done\n",
      "epoch 240: update done\n",
      "Loss at 240: 0.560147\n",
      "epoch 240 test accuracy: 0.9913333333333333\n",
      "epoch 241: pred done\n",
      "epoch 241: backward done\n",
      "epoch 241: update done\n",
      "Loss at 241: 0.560136\n",
      "epoch 241 test accuracy: 0.9913333333333333\n",
      "epoch 242: pred done\n",
      "epoch 242: backward done\n",
      "epoch 242: update done\n",
      "Loss at 242: 0.560124\n",
      "epoch 242 test accuracy: 0.9913333333333333\n",
      "epoch 243: pred done\n",
      "epoch 243: backward done\n",
      "epoch 243: update done\n",
      "Loss at 243: 0.560113\n",
      "epoch 243 test accuracy: 0.9913333333333333\n",
      "epoch 244: pred done\n",
      "epoch 244: backward done\n",
      "epoch 244: update done\n",
      "Loss at 244: 0.560102\n",
      "epoch 244 test accuracy: 0.9913333333333333\n",
      "epoch 245: pred done\n",
      "epoch 245: backward done\n",
      "epoch 245: update done\n",
      "Loss at 245: 0.560091\n",
      "epoch 245 test accuracy: 0.9913333333333333\n",
      "epoch 246: pred done\n",
      "epoch 246: backward done\n",
      "epoch 246: update done\n",
      "Loss at 246: 0.560079\n",
      "epoch 246 test accuracy: 0.9913333333333333\n",
      "epoch 247: pred done\n",
      "epoch 247: backward done\n",
      "epoch 247: update done\n",
      "Loss at 247: 0.560068\n",
      "epoch 247 test accuracy: 0.9913333333333333\n",
      "epoch 248: pred done\n",
      "epoch 248: backward done\n",
      "epoch 248: update done\n",
      "Loss at 248: 0.560057\n",
      "epoch 248 test accuracy: 0.9913333333333333\n",
      "epoch 249: pred done\n",
      "epoch 249: backward done\n",
      "epoch 249: update done\n",
      "Loss at 249: 0.560046\n",
      "epoch 249 test accuracy: 0.9913333333333333\n",
      "epoch 250: pred done\n",
      "epoch 250: backward done\n",
      "epoch 250: update done\n",
      "Loss at 250: 0.560035\n",
      "epoch 250 test accuracy: 0.9913333333333333\n",
      "epoch 251: pred done\n",
      "epoch 251: backward done\n",
      "epoch 251: update done\n",
      "Loss at 251: 0.560024\n",
      "epoch 251 test accuracy: 0.9913333333333333\n",
      "epoch 252: pred done\n",
      "epoch 252: backward done\n",
      "epoch 252: update done\n",
      "Loss at 252: 0.560014\n",
      "epoch 252 test accuracy: 0.9913333333333333\n",
      "epoch 253: pred done\n",
      "epoch 253: backward done\n",
      "epoch 253: update done\n",
      "Loss at 253: 0.560003\n",
      "epoch 253 test accuracy: 0.9913333333333333\n",
      "epoch 254: pred done\n",
      "epoch 254: backward done\n",
      "epoch 254: update done\n",
      "Loss at 254: 0.559992\n",
      "epoch 254 test accuracy: 0.9913333333333333\n",
      "epoch 255: pred done\n",
      "epoch 255: backward done\n",
      "epoch 255: update done\n",
      "Loss at 255: 0.559981\n",
      "epoch 255 test accuracy: 0.9913333333333333\n",
      "epoch 256: pred done\n",
      "epoch 256: backward done\n",
      "epoch 256: update done\n",
      "Loss at 256: 0.559971\n",
      "epoch 256 test accuracy: 0.9913333333333333\n",
      "epoch 257: pred done\n",
      "epoch 257: backward done\n",
      "epoch 257: update done\n",
      "Loss at 257: 0.559960\n",
      "epoch 257 test accuracy: 0.9913333333333333\n",
      "epoch 258: pred done\n",
      "epoch 258: backward done\n",
      "epoch 258: update done\n",
      "Loss at 258: 0.559950\n",
      "epoch 258 test accuracy: 0.9913333333333333\n",
      "epoch 259: pred done\n",
      "epoch 259: backward done\n",
      "epoch 259: update done\n",
      "Loss at 259: 0.559939\n",
      "epoch 259 test accuracy: 0.9913333333333333\n",
      "epoch 260: pred done\n",
      "epoch 260: backward done\n",
      "epoch 260: update done\n",
      "Loss at 260: 0.559929\n",
      "epoch 260 test accuracy: 0.9913333333333333\n",
      "epoch 261: pred done\n",
      "epoch 261: backward done\n",
      "epoch 261: update done\n",
      "Loss at 261: 0.559918\n",
      "epoch 261 test accuracy: 0.9913333333333333\n",
      "epoch 262: pred done\n",
      "epoch 262: backward done\n",
      "epoch 262: update done\n",
      "Loss at 262: 0.559908\n",
      "epoch 262 test accuracy: 0.9916666666666667\n",
      "epoch 263: pred done\n",
      "epoch 263: backward done\n",
      "epoch 263: update done\n",
      "Loss at 263: 0.559898\n",
      "epoch 263 test accuracy: 0.9916666666666667\n",
      "epoch 264: pred done\n",
      "epoch 264: backward done\n",
      "epoch 264: update done\n",
      "Loss at 264: 0.559888\n",
      "epoch 264 test accuracy: 0.9916666666666667\n",
      "epoch 265: pred done\n",
      "epoch 265: backward done\n",
      "epoch 265: update done\n",
      "Loss at 265: 0.559877\n",
      "epoch 265 test accuracy: 0.9916666666666667\n",
      "epoch 266: pred done\n",
      "epoch 266: backward done\n",
      "epoch 266: update done\n",
      "Loss at 266: 0.559867\n",
      "epoch 266 test accuracy: 0.9916666666666667\n",
      "epoch 267: pred done\n",
      "epoch 267: backward done\n",
      "epoch 267: update done\n",
      "Loss at 267: 0.559857\n",
      "epoch 267 test accuracy: 0.9916666666666667\n",
      "epoch 268: pred done\n",
      "epoch 268: backward done\n",
      "epoch 268: update done\n",
      "Loss at 268: 0.559847\n",
      "epoch 268 test accuracy: 0.9916666666666667\n",
      "epoch 269: pred done\n",
      "epoch 269: backward done\n",
      "epoch 269: update done\n",
      "Loss at 269: 0.559837\n",
      "epoch 269 test accuracy: 0.9916666666666667\n",
      "epoch 270: pred done\n",
      "epoch 270: backward done\n",
      "epoch 270: update done\n",
      "Loss at 270: 0.559827\n",
      "epoch 270 test accuracy: 0.9916666666666667\n",
      "epoch 271: pred done\n",
      "epoch 271: backward done\n",
      "epoch 271: update done\n",
      "Loss at 271: 0.559817\n",
      "epoch 271 test accuracy: 0.9916666666666667\n",
      "epoch 272: pred done\n",
      "epoch 272: backward done\n",
      "epoch 272: update done\n",
      "Loss at 272: 0.559807\n",
      "epoch 272 test accuracy: 0.9916666666666667\n",
      "epoch 273: pred done\n",
      "epoch 273: backward done\n",
      "epoch 273: update done\n",
      "Loss at 273: 0.559797\n",
      "epoch 273 test accuracy: 0.9916666666666667\n",
      "epoch 274: pred done\n",
      "epoch 274: backward done\n",
      "epoch 274: update done\n",
      "Loss at 274: 0.559787\n",
      "epoch 274 test accuracy: 0.9916666666666667\n",
      "epoch 275: pred done\n",
      "epoch 275: backward done\n",
      "epoch 275: update done\n",
      "Loss at 275: 0.559777\n",
      "epoch 275 test accuracy: 0.9916666666666667\n",
      "epoch 276: pred done\n",
      "epoch 276: backward done\n",
      "epoch 276: update done\n",
      "Loss at 276: 0.559767\n",
      "epoch 276 test accuracy: 0.9916666666666667\n",
      "epoch 277: pred done\n",
      "epoch 277: backward done\n",
      "epoch 277: update done\n",
      "Loss at 277: 0.559758\n",
      "epoch 277 test accuracy: 0.992\n",
      "epoch 278: pred done\n",
      "epoch 278: backward done\n",
      "epoch 278: update done\n",
      "Loss at 278: 0.559748\n",
      "epoch 278 test accuracy: 0.992\n",
      "epoch 279: pred done\n",
      "epoch 279: backward done\n",
      "epoch 279: update done\n",
      "Loss at 279: 0.559738\n",
      "epoch 279 test accuracy: 0.992\n",
      "epoch 280: pred done\n",
      "epoch 280: backward done\n",
      "epoch 280: update done\n",
      "Loss at 280: 0.559729\n",
      "epoch 280 test accuracy: 0.992\n",
      "epoch 281: pred done\n",
      "epoch 281: backward done\n",
      "epoch 281: update done\n",
      "Loss at 281: 0.559719\n",
      "epoch 281 test accuracy: 0.992\n",
      "epoch 282: pred done\n",
      "epoch 282: backward done\n",
      "epoch 282: update done\n",
      "Loss at 282: 0.559710\n",
      "epoch 282 test accuracy: 0.992\n",
      "epoch 283: pred done\n",
      "epoch 283: backward done\n",
      "epoch 283: update done\n",
      "Loss at 283: 0.559700\n",
      "epoch 283 test accuracy: 0.992\n",
      "epoch 284: pred done\n",
      "epoch 284: backward done\n",
      "epoch 284: update done\n",
      "Loss at 284: 0.559691\n",
      "epoch 284 test accuracy: 0.992\n",
      "epoch 285: pred done\n",
      "epoch 285: backward done\n",
      "epoch 285: update done\n",
      "Loss at 285: 0.559681\n",
      "epoch 285 test accuracy: 0.992\n",
      "epoch 286: pred done\n",
      "epoch 286: backward done\n",
      "epoch 286: update done\n",
      "Loss at 286: 0.559672\n",
      "epoch 286 test accuracy: 0.992\n",
      "epoch 287: pred done\n",
      "epoch 287: backward done\n",
      "epoch 287: update done\n",
      "Loss at 287: 0.559663\n",
      "epoch 287 test accuracy: 0.992\n",
      "epoch 288: pred done\n",
      "epoch 288: backward done\n",
      "epoch 288: update done\n",
      "Loss at 288: 0.559653\n",
      "epoch 288 test accuracy: 0.992\n",
      "epoch 289: pred done\n",
      "epoch 289: backward done\n",
      "epoch 289: update done\n",
      "Loss at 289: 0.559644\n",
      "epoch 289 test accuracy: 0.992\n",
      "epoch 290: pred done\n",
      "epoch 290: backward done\n",
      "epoch 290: update done\n",
      "Loss at 290: 0.559635\n",
      "epoch 290 test accuracy: 0.992\n",
      "epoch 291: pred done\n",
      "epoch 291: backward done\n",
      "epoch 291: update done\n",
      "Loss at 291: 0.559626\n",
      "epoch 291 test accuracy: 0.9923333333333333\n",
      "epoch 292: pred done\n",
      "epoch 292: backward done\n",
      "epoch 292: update done\n",
      "Loss at 292: 0.559617\n",
      "epoch 292 test accuracy: 0.9923333333333333\n",
      "epoch 293: pred done\n",
      "epoch 293: backward done\n",
      "epoch 293: update done\n",
      "Loss at 293: 0.559607\n",
      "epoch 293 test accuracy: 0.9923333333333333\n",
      "epoch 294: pred done\n",
      "epoch 294: backward done\n",
      "epoch 294: update done\n",
      "Loss at 294: 0.559598\n",
      "epoch 294 test accuracy: 0.9923333333333333\n",
      "epoch 295: pred done\n",
      "epoch 295: backward done\n",
      "epoch 295: update done\n",
      "Loss at 295: 0.559589\n",
      "epoch 295 test accuracy: 0.9923333333333333\n",
      "epoch 296: pred done\n",
      "epoch 296: backward done\n",
      "epoch 296: update done\n",
      "Loss at 296: 0.559580\n",
      "epoch 296 test accuracy: 0.9923333333333333\n",
      "epoch 297: pred done\n",
      "epoch 297: backward done\n",
      "epoch 297: update done\n",
      "Loss at 297: 0.559571\n",
      "epoch 297 test accuracy: 0.9923333333333333\n",
      "epoch 298: pred done\n",
      "epoch 298: backward done\n",
      "epoch 298: update done\n",
      "Loss at 298: 0.559562\n",
      "epoch 298 test accuracy: 0.9923333333333333\n",
      "epoch 299: pred done\n",
      "epoch 299: backward done\n",
      "epoch 299: update done\n",
      "Loss at 299: 0.559553\n",
      "epoch 299 test accuracy: 0.9923333333333333\n"
     ]
    }
   ],
   "source": [
    "model2 = MLP([Linear(784, 50, seed=43), ReLU(), Linear(50, 50, seed=44), ReLU(), Linear(50, 3, seed=45),\n",
    "             Sigmoid(), \n",
    "             SoftMaxLayer()], CELoss(), \n",
    "            #  GradientDescent(ic(1*(y_train.shape[0]))),\n",
    "             GradientDescent(0.8),\n",
    "            #  GradientDescent(0.001),\n",
    "             )\n",
    "#############################\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# Train the network using only `x_train` and `y_train` (no validation)\n",
    "train(model2, epochs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model2, epochs, x_train, y_train, start_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "m1lSq2jNcdnY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: pred done\n",
      "epoch 0: backward done\n",
      "epoch 0: update done\n",
      "Loss at 0: 1.628290\n",
      "epoch 0 test accuracy: 0.5226666666666666\n",
      "epoch 1: pred done\n",
      "epoch 1: backward done\n",
      "epoch 1: update done\n",
      "Loss at 1: 0.931209\n",
      "epoch 1 test accuracy: 0.828\n",
      "epoch 2: pred done\n",
      "epoch 2: backward done\n",
      "epoch 2: update done\n",
      "Loss at 2: 0.507214\n",
      "epoch 2 test accuracy: 0.7666666666666667\n",
      "epoch 3: pred done\n",
      "epoch 3: backward done\n",
      "epoch 3: update done\n",
      "Loss at 3: 0.421531\n",
      "epoch 3 test accuracy: 0.684\n",
      "epoch 4: pred done\n",
      "epoch 4: backward done\n",
      "epoch 4: update done\n",
      "Loss at 4: 0.780244\n",
      "epoch 4 test accuracy: 0.7006666666666667\n",
      "epoch 5: pred done\n",
      "epoch 5: backward done\n",
      "epoch 5: update done\n",
      "Loss at 5: 0.489754\n",
      "epoch 5 test accuracy: 0.873\n",
      "epoch 6: pred done\n",
      "epoch 6: backward done\n",
      "epoch 6: update done\n",
      "Loss at 6: 0.283852\n",
      "epoch 6 test accuracy: 0.9593333333333334\n",
      "epoch 7: pred done\n",
      "epoch 7: backward done\n",
      "epoch 7: update done\n",
      "Loss at 7: 0.178877\n",
      "epoch 7 test accuracy: 0.9663333333333334\n",
      "epoch 8: pred done\n",
      "epoch 8: backward done\n",
      "epoch 8: update done\n",
      "Loss at 8: 0.142072\n",
      "epoch 8 test accuracy: 0.9706666666666667\n",
      "epoch 9: pred done\n",
      "epoch 9: backward done\n",
      "epoch 9: update done\n",
      "Loss at 9: 0.120561\n",
      "epoch 9 test accuracy: 0.972\n",
      "epoch 10: pred done\n",
      "epoch 10: backward done\n",
      "epoch 10: update done\n",
      "Loss at 10: 0.106960\n",
      "epoch 10 test accuracy: 0.971\n",
      "epoch 11: pred done\n",
      "epoch 11: backward done\n",
      "epoch 11: update done\n",
      "Loss at 11: 0.097823\n",
      "epoch 11 test accuracy: 0.971\n",
      "epoch 12: pred done\n",
      "epoch 12: backward done\n",
      "epoch 12: update done\n",
      "Loss at 12: 0.091348\n",
      "epoch 12 test accuracy: 0.972\n",
      "epoch 13: pred done\n",
      "epoch 13: backward done\n",
      "epoch 13: update done\n",
      "Loss at 13: 0.086468\n",
      "epoch 13 test accuracy: 0.9723333333333334\n",
      "epoch 14: pred done\n",
      "epoch 14: backward done\n",
      "epoch 14: update done\n",
      "Loss at 14: 0.082579\n",
      "epoch 14 test accuracy: 0.973\n",
      "epoch 15: pred done\n",
      "epoch 15: backward done\n",
      "epoch 15: update done\n",
      "Loss at 15: 0.079374\n",
      "epoch 15 test accuracy: 0.9726666666666667\n",
      "epoch 16: pred done\n",
      "epoch 16: backward done\n",
      "epoch 16: update done\n",
      "Loss at 16: 0.076635\n",
      "epoch 16 test accuracy: 0.973\n",
      "epoch 17: pred done\n",
      "epoch 17: backward done\n",
      "epoch 17: update done\n",
      "Loss at 17: 0.074265\n",
      "epoch 17 test accuracy: 0.9736666666666667\n",
      "epoch 18: pred done\n",
      "epoch 18: backward done\n",
      "epoch 18: update done\n",
      "Loss at 18: 0.072177\n",
      "epoch 18 test accuracy: 0.974\n",
      "epoch 19: pred done\n",
      "epoch 19: backward done\n",
      "epoch 19: update done\n",
      "Loss at 19: 0.070309\n",
      "epoch 19 test accuracy: 0.974\n",
      "epoch 20: pred done\n",
      "epoch 20: backward done\n",
      "epoch 20: update done\n",
      "Loss at 20: 0.068623\n",
      "epoch 20 test accuracy: 0.9746666666666667\n",
      "epoch 21: pred done\n",
      "epoch 21: backward done\n",
      "epoch 21: update done\n",
      "Loss at 21: 0.067083\n",
      "epoch 21 test accuracy: 0.9753333333333334\n",
      "epoch 22: pred done\n",
      "epoch 22: backward done\n",
      "epoch 22: update done\n",
      "Loss at 22: 0.065666\n",
      "epoch 22 test accuracy: 0.976\n",
      "epoch 23: pred done\n",
      "epoch 23: backward done\n",
      "epoch 23: update done\n",
      "Loss at 23: 0.064350\n",
      "epoch 23 test accuracy: 0.977\n",
      "epoch 24: pred done\n",
      "epoch 24: backward done\n",
      "epoch 24: update done\n",
      "Loss at 24: 0.063125\n",
      "epoch 24 test accuracy: 0.9783333333333334\n",
      "epoch 25: pred done\n",
      "epoch 25: backward done\n",
      "epoch 25: update done\n",
      "Loss at 25: 0.061981\n",
      "epoch 25 test accuracy: 0.9783333333333334\n",
      "epoch 26: pred done\n",
      "epoch 26: backward done\n",
      "epoch 26: update done\n",
      "Loss at 26: 0.060911\n",
      "epoch 26 test accuracy: 0.979\n",
      "epoch 27: pred done\n",
      "epoch 27: backward done\n",
      "epoch 27: update done\n",
      "Loss at 27: 0.059904\n",
      "epoch 27 test accuracy: 0.9793333333333333\n",
      "epoch 28: pred done\n",
      "epoch 28: backward done\n",
      "epoch 28: update done\n",
      "Loss at 28: 0.058955\n",
      "epoch 28 test accuracy: 0.9796666666666667\n",
      "epoch 29: pred done\n",
      "epoch 29: backward done\n",
      "epoch 29: update done\n",
      "Loss at 29: 0.058058\n",
      "epoch 29 test accuracy: 0.981\n",
      "epoch 30: pred done\n",
      "epoch 30: backward done\n",
      "epoch 30: update done\n",
      "Loss at 30: 0.057210\n",
      "epoch 30 test accuracy: 0.9813333333333333\n",
      "epoch 31: pred done\n",
      "epoch 31: backward done\n",
      "epoch 31: update done\n",
      "Loss at 31: 0.056404\n",
      "epoch 31 test accuracy: 0.9823333333333333\n",
      "epoch 32: pred done\n",
      "epoch 32: backward done\n",
      "epoch 32: update done\n",
      "Loss at 32: 0.055638\n",
      "epoch 32 test accuracy: 0.9826666666666667\n",
      "epoch 33: pred done\n",
      "epoch 33: backward done\n",
      "epoch 33: update done\n",
      "Loss at 33: 0.054911\n",
      "epoch 33 test accuracy: 0.983\n",
      "epoch 34: pred done\n",
      "epoch 34: backward done\n",
      "epoch 34: update done\n",
      "Loss at 34: 0.054217\n",
      "epoch 34 test accuracy: 0.983\n",
      "epoch 35: pred done\n",
      "epoch 35: backward done\n",
      "epoch 35: update done\n",
      "Loss at 35: 0.053556\n",
      "epoch 35 test accuracy: 0.983\n",
      "epoch 36: pred done\n",
      "epoch 36: backward done\n",
      "epoch 36: update done\n",
      "Loss at 36: 0.052927\n",
      "epoch 36 test accuracy: 0.9833333333333333\n",
      "epoch 37: pred done\n",
      "epoch 37: backward done\n",
      "epoch 37: update done\n",
      "Loss at 37: 0.052326\n",
      "epoch 37 test accuracy: 0.9836666666666667\n",
      "epoch 38: pred done\n",
      "epoch 38: backward done\n",
      "epoch 38: update done\n",
      "Loss at 38: 0.051750\n",
      "epoch 38 test accuracy: 0.984\n",
      "epoch 39: pred done\n",
      "epoch 39: backward done\n",
      "epoch 39: update done\n",
      "Loss at 39: 0.051197\n",
      "epoch 39 test accuracy: 0.9843333333333333\n",
      "epoch 40: pred done\n",
      "epoch 40: backward done\n",
      "epoch 40: update done\n",
      "Loss at 40: 0.050667\n",
      "epoch 40 test accuracy: 0.9843333333333333\n",
      "epoch 41: pred done\n",
      "epoch 41: backward done\n",
      "epoch 41: update done\n",
      "Loss at 41: 0.050159\n",
      "epoch 41 test accuracy: 0.9846666666666667\n",
      "epoch 42: pred done\n",
      "epoch 42: backward done\n",
      "epoch 42: update done\n",
      "Loss at 42: 0.049670\n",
      "epoch 42 test accuracy: 0.9846666666666667\n",
      "epoch 43: pred done\n",
      "epoch 43: backward done\n",
      "epoch 43: update done\n",
      "Loss at 43: 0.049199\n",
      "epoch 43 test accuracy: 0.985\n",
      "epoch 44: pred done\n",
      "epoch 44: backward done\n",
      "epoch 44: update done\n",
      "Loss at 44: 0.048747\n",
      "epoch 44 test accuracy: 0.985\n",
      "epoch 45: pred done\n",
      "epoch 45: backward done\n",
      "epoch 45: update done\n",
      "Loss at 45: 0.048311\n",
      "epoch 45 test accuracy: 0.985\n",
      "epoch 46: pred done\n",
      "epoch 46: backward done\n",
      "epoch 46: update done\n",
      "Loss at 46: 0.047889\n",
      "epoch 46 test accuracy: 0.985\n",
      "epoch 47: pred done\n",
      "epoch 47: backward done\n",
      "epoch 47: update done\n",
      "Loss at 47: 0.047481\n",
      "epoch 47 test accuracy: 0.985\n",
      "epoch 48: pred done\n",
      "epoch 48: backward done\n",
      "epoch 48: update done\n",
      "Loss at 48: 0.047088\n",
      "epoch 48 test accuracy: 0.985\n",
      "epoch 49: pred done\n",
      "epoch 49: backward done\n",
      "epoch 49: update done\n",
      "Loss at 49: 0.046711\n",
      "epoch 49 test accuracy: 0.9856666666666667\n",
      "epoch 50: pred done\n",
      "epoch 50: backward done\n",
      "epoch 50: update done\n",
      "Loss at 50: 0.046348\n",
      "epoch 50 test accuracy: 0.9856666666666667\n",
      "epoch 51: pred done\n",
      "epoch 51: backward done\n",
      "epoch 51: update done\n",
      "Loss at 51: 0.045997\n",
      "epoch 51 test accuracy: 0.986\n",
      "epoch 52: pred done\n",
      "epoch 52: backward done\n",
      "epoch 52: update done\n",
      "Loss at 52: 0.045658\n",
      "epoch 52 test accuracy: 0.986\n",
      "epoch 53: pred done\n",
      "epoch 53: backward done\n",
      "epoch 53: update done\n",
      "Loss at 53: 0.045329\n",
      "epoch 53 test accuracy: 0.986\n",
      "epoch 54: pred done\n",
      "epoch 54: backward done\n",
      "epoch 54: update done\n",
      "Loss at 54: 0.045011\n",
      "epoch 54 test accuracy: 0.986\n",
      "epoch 55: pred done\n",
      "epoch 55: backward done\n",
      "epoch 55: update done\n",
      "Loss at 55: 0.044703\n",
      "epoch 55 test accuracy: 0.986\n",
      "epoch 56: pred done\n",
      "epoch 56: backward done\n",
      "epoch 56: update done\n",
      "Loss at 56: 0.044401\n",
      "epoch 56 test accuracy: 0.986\n",
      "epoch 57: pred done\n",
      "epoch 57: backward done\n",
      "epoch 57: update done\n",
      "Loss at 57: 0.044106\n",
      "epoch 57 test accuracy: 0.986\n",
      "epoch 58: pred done\n",
      "epoch 58: backward done\n",
      "epoch 58: update done\n",
      "Loss at 58: 0.043821\n",
      "epoch 58 test accuracy: 0.986\n",
      "epoch 59: pred done\n",
      "epoch 59: backward done\n",
      "epoch 59: update done\n",
      "Loss at 59: 0.043545\n",
      "epoch 59 test accuracy: 0.986\n",
      "epoch 60: pred done\n",
      "epoch 60: backward done\n",
      "epoch 60: update done\n",
      "Loss at 60: 0.043275\n",
      "epoch 60 test accuracy: 0.9866666666666667\n",
      "epoch 61: pred done\n",
      "epoch 61: backward done\n",
      "epoch 61: update done\n",
      "Loss at 61: 0.043013\n",
      "epoch 61 test accuracy: 0.9866666666666667\n",
      "epoch 62: pred done\n",
      "epoch 62: backward done\n",
      "epoch 62: update done\n",
      "Loss at 62: 0.042759\n",
      "epoch 62 test accuracy: 0.987\n",
      "epoch 63: pred done\n",
      "epoch 63: backward done\n",
      "epoch 63: update done\n",
      "Loss at 63: 0.042511\n",
      "epoch 63 test accuracy: 0.987\n",
      "epoch 64: pred done\n",
      "epoch 64: backward done\n",
      "epoch 64: update done\n",
      "Loss at 64: 0.042269\n",
      "epoch 64 test accuracy: 0.987\n",
      "epoch 65: pred done\n",
      "epoch 65: backward done\n",
      "epoch 65: update done\n",
      "Loss at 65: 0.042034\n",
      "epoch 65 test accuracy: 0.987\n",
      "epoch 66: pred done\n",
      "epoch 66: backward done\n",
      "epoch 66: update done\n",
      "Loss at 66: 0.041805\n",
      "epoch 66 test accuracy: 0.987\n",
      "epoch 67: pred done\n",
      "epoch 67: backward done\n",
      "epoch 67: update done\n",
      "Loss at 67: 0.041581\n",
      "epoch 67 test accuracy: 0.9873333333333333\n",
      "epoch 68: pred done\n",
      "epoch 68: backward done\n",
      "epoch 68: update done\n",
      "Loss at 68: 0.041361\n",
      "epoch 68 test accuracy: 0.9873333333333333\n",
      "epoch 69: pred done\n",
      "epoch 69: backward done\n",
      "epoch 69: update done\n",
      "Loss at 69: 0.041147\n",
      "epoch 69 test accuracy: 0.9873333333333333\n",
      "epoch 70: pred done\n",
      "epoch 70: backward done\n",
      "epoch 70: update done\n",
      "Loss at 70: 0.040937\n",
      "epoch 70 test accuracy: 0.9873333333333333\n",
      "epoch 71: pred done\n",
      "epoch 71: backward done\n",
      "epoch 71: update done\n",
      "Loss at 71: 0.040731\n",
      "epoch 71 test accuracy: 0.9873333333333333\n",
      "epoch 72: pred done\n",
      "epoch 72: backward done\n",
      "epoch 72: update done\n",
      "Loss at 72: 0.040530\n",
      "epoch 72 test accuracy: 0.9873333333333333\n",
      "epoch 73: pred done\n",
      "epoch 73: backward done\n",
      "epoch 73: update done\n",
      "Loss at 73: 0.040334\n",
      "epoch 73 test accuracy: 0.9873333333333333\n",
      "epoch 74: pred done\n",
      "epoch 74: backward done\n",
      "epoch 74: update done\n",
      "Loss at 74: 0.040143\n",
      "epoch 74 test accuracy: 0.9873333333333333\n",
      "epoch 75: pred done\n",
      "epoch 75: backward done\n",
      "epoch 75: update done\n",
      "Loss at 75: 0.039957\n",
      "epoch 75 test accuracy: 0.9873333333333333\n",
      "epoch 76: pred done\n",
      "epoch 76: backward done\n",
      "epoch 76: update done\n",
      "Loss at 76: 0.039775\n",
      "epoch 76 test accuracy: 0.9873333333333333\n",
      "epoch 77: pred done\n",
      "epoch 77: backward done\n",
      "epoch 77: update done\n",
      "Loss at 77: 0.039595\n",
      "epoch 77 test accuracy: 0.9873333333333333\n",
      "epoch 78: pred done\n",
      "epoch 78: backward done\n",
      "epoch 78: update done\n",
      "Loss at 78: 0.039418\n",
      "epoch 78 test accuracy: 0.9873333333333333\n",
      "epoch 79: pred done\n",
      "epoch 79: backward done\n",
      "epoch 79: update done\n",
      "Loss at 79: 0.039244\n",
      "epoch 79 test accuracy: 0.9876666666666667\n",
      "epoch 80: pred done\n",
      "epoch 80: backward done\n",
      "epoch 80: update done\n",
      "Loss at 80: 0.039074\n",
      "epoch 80 test accuracy: 0.9876666666666667\n",
      "epoch 81: pred done\n",
      "epoch 81: backward done\n",
      "epoch 81: update done\n",
      "Loss at 81: 0.038906\n",
      "epoch 81 test accuracy: 0.9876666666666667\n",
      "epoch 82: pred done\n",
      "epoch 82: backward done\n",
      "epoch 82: update done\n",
      "Loss at 82: 0.038742\n",
      "epoch 82 test accuracy: 0.9876666666666667\n",
      "epoch 83: pred done\n",
      "epoch 83: backward done\n",
      "epoch 83: update done\n",
      "Loss at 83: 0.038580\n",
      "epoch 83 test accuracy: 0.9876666666666667\n",
      "epoch 84: pred done\n",
      "epoch 84: backward done\n",
      "epoch 84: update done\n",
      "Loss at 84: 0.038421\n",
      "epoch 84 test accuracy: 0.9876666666666667\n",
      "epoch 85: pred done\n",
      "epoch 85: backward done\n",
      "epoch 85: update done\n",
      "Loss at 85: 0.038265\n",
      "epoch 85 test accuracy: 0.9876666666666667\n",
      "epoch 86: pred done\n",
      "epoch 86: backward done\n",
      "epoch 86: update done\n",
      "Loss at 86: 0.038110\n",
      "epoch 86 test accuracy: 0.9876666666666667\n",
      "epoch 87: pred done\n",
      "epoch 87: backward done\n",
      "epoch 87: update done\n",
      "Loss at 87: 0.037957\n",
      "epoch 87 test accuracy: 0.9876666666666667\n",
      "epoch 88: pred done\n",
      "epoch 88: backward done\n",
      "epoch 88: update done\n",
      "Loss at 88: 0.037805\n",
      "epoch 88 test accuracy: 0.9876666666666667\n",
      "epoch 89: pred done\n",
      "epoch 89: backward done\n",
      "epoch 89: update done\n",
      "Loss at 89: 0.037654\n",
      "epoch 89 test accuracy: 0.9876666666666667\n",
      "epoch 90: pred done\n",
      "epoch 90: backward done\n",
      "epoch 90: update done\n",
      "Loss at 90: 0.037506\n",
      "epoch 90 test accuracy: 0.988\n",
      "epoch 91: pred done\n",
      "epoch 91: backward done\n",
      "epoch 91: update done\n",
      "Loss at 91: 0.037361\n",
      "epoch 91 test accuracy: 0.988\n",
      "epoch 92: pred done\n",
      "epoch 92: backward done\n",
      "epoch 92: update done\n",
      "Loss at 92: 0.037217\n",
      "epoch 92 test accuracy: 0.988\n",
      "epoch 93: pred done\n",
      "epoch 93: backward done\n",
      "epoch 93: update done\n",
      "Loss at 93: 0.037076\n",
      "epoch 93 test accuracy: 0.988\n",
      "epoch 94: pred done\n",
      "epoch 94: backward done\n",
      "epoch 94: update done\n",
      "Loss at 94: 0.036937\n",
      "epoch 94 test accuracy: 0.988\n",
      "epoch 95: pred done\n",
      "epoch 95: backward done\n",
      "epoch 95: update done\n",
      "Loss at 95: 0.036801\n",
      "epoch 95 test accuracy: 0.988\n",
      "epoch 96: pred done\n",
      "epoch 96: backward done\n",
      "epoch 96: update done\n",
      "Loss at 96: 0.036666\n",
      "epoch 96 test accuracy: 0.988\n",
      "epoch 97: pred done\n",
      "epoch 97: backward done\n",
      "epoch 97: update done\n",
      "Loss at 97: 0.036536\n",
      "epoch 97 test accuracy: 0.988\n",
      "epoch 98: pred done\n",
      "epoch 98: backward done\n",
      "epoch 98: update done\n",
      "Loss at 98: 0.036407\n",
      "epoch 98 test accuracy: 0.988\n",
      "epoch 99: pred done\n",
      "epoch 99: backward done\n",
      "epoch 99: update done\n",
      "Loss at 99: 0.036281\n",
      "epoch 99 test accuracy: 0.988\n",
      "epoch 100: pred done\n",
      "epoch 100: backward done\n",
      "epoch 100: update done\n",
      "Loss at 100: 0.036157\n",
      "epoch 100 test accuracy: 0.9883333333333333\n",
      "epoch 101: pred done\n",
      "epoch 101: backward done\n",
      "epoch 101: update done\n",
      "Loss at 101: 0.036035\n",
      "epoch 101 test accuracy: 0.9886666666666667\n",
      "epoch 102: pred done\n",
      "epoch 102: backward done\n",
      "epoch 102: update done\n",
      "Loss at 102: 0.035915\n",
      "epoch 102 test accuracy: 0.9886666666666667\n",
      "epoch 103: pred done\n",
      "epoch 103: backward done\n",
      "epoch 103: update done\n",
      "Loss at 103: 0.035796\n",
      "epoch 103 test accuracy: 0.9886666666666667\n",
      "epoch 104: pred done\n",
      "epoch 104: backward done\n",
      "epoch 104: update done\n",
      "Loss at 104: 0.035679\n",
      "epoch 104 test accuracy: 0.9886666666666667\n",
      "epoch 105: pred done\n",
      "epoch 105: backward done\n",
      "epoch 105: update done\n",
      "Loss at 105: 0.035565\n",
      "epoch 105 test accuracy: 0.9886666666666667\n",
      "epoch 106: pred done\n",
      "epoch 106: backward done\n",
      "epoch 106: update done\n",
      "Loss at 106: 0.035453\n",
      "epoch 106 test accuracy: 0.9886666666666667\n",
      "epoch 107: pred done\n",
      "epoch 107: backward done\n",
      "epoch 107: update done\n",
      "Loss at 107: 0.035342\n",
      "epoch 107 test accuracy: 0.9886666666666667\n",
      "epoch 108: pred done\n",
      "epoch 108: backward done\n",
      "epoch 108: update done\n",
      "Loss at 108: 0.035232\n",
      "epoch 108 test accuracy: 0.9886666666666667\n",
      "epoch 109: pred done\n",
      "epoch 109: backward done\n",
      "epoch 109: update done\n",
      "Loss at 109: 0.035124\n",
      "epoch 109 test accuracy: 0.9886666666666667\n",
      "epoch 110: pred done\n",
      "epoch 110: backward done\n",
      "epoch 110: update done\n",
      "Loss at 110: 0.035018\n",
      "epoch 110 test accuracy: 0.9886666666666667\n",
      "epoch 111: pred done\n",
      "epoch 111: backward done\n",
      "epoch 111: update done\n",
      "Loss at 111: 0.034912\n",
      "epoch 111 test accuracy: 0.9886666666666667\n",
      "epoch 112: pred done\n",
      "epoch 112: backward done\n",
      "epoch 112: update done\n",
      "Loss at 112: 0.034808\n",
      "epoch 112 test accuracy: 0.9886666666666667\n",
      "epoch 113: pred done\n",
      "epoch 113: backward done\n",
      "epoch 113: update done\n",
      "Loss at 113: 0.034705\n",
      "epoch 113 test accuracy: 0.9886666666666667\n",
      "epoch 114: pred done\n",
      "epoch 114: backward done\n",
      "epoch 114: update done\n",
      "Loss at 114: 0.034604\n",
      "epoch 114 test accuracy: 0.9886666666666667\n",
      "epoch 115: pred done\n",
      "epoch 115: backward done\n",
      "epoch 115: update done\n",
      "Loss at 115: 0.034504\n",
      "epoch 115 test accuracy: 0.9886666666666667\n",
      "epoch 116: pred done\n",
      "epoch 116: backward done\n",
      "epoch 116: update done\n",
      "Loss at 116: 0.034405\n",
      "epoch 116 test accuracy: 0.9886666666666667\n",
      "epoch 117: pred done\n",
      "epoch 117: backward done\n",
      "epoch 117: update done\n",
      "Loss at 117: 0.034307\n",
      "epoch 117 test accuracy: 0.9886666666666667\n",
      "epoch 118: pred done\n",
      "epoch 118: backward done\n",
      "epoch 118: update done\n",
      "Loss at 118: 0.034210\n",
      "epoch 118 test accuracy: 0.9886666666666667\n",
      "epoch 119: pred done\n",
      "epoch 119: backward done\n",
      "epoch 119: update done\n",
      "Loss at 119: 0.034114\n",
      "epoch 119 test accuracy: 0.989\n",
      "epoch 120: pred done\n",
      "epoch 120: backward done\n",
      "epoch 120: update done\n",
      "Loss at 120: 0.034020\n",
      "epoch 120 test accuracy: 0.989\n",
      "epoch 121: pred done\n",
      "epoch 121: backward done\n",
      "epoch 121: update done\n",
      "Loss at 121: 0.033927\n",
      "epoch 121 test accuracy: 0.989\n",
      "epoch 122: pred done\n",
      "epoch 122: backward done\n",
      "epoch 122: update done\n",
      "Loss at 122: 0.033834\n",
      "epoch 122 test accuracy: 0.989\n",
      "epoch 123: pred done\n",
      "epoch 123: backward done\n",
      "epoch 123: update done\n",
      "Loss at 123: 0.033742\n",
      "epoch 123 test accuracy: 0.989\n",
      "epoch 124: pred done\n",
      "epoch 124: backward done\n",
      "epoch 124: update done\n",
      "Loss at 124: 0.033651\n",
      "epoch 124 test accuracy: 0.989\n",
      "epoch 125: pred done\n",
      "epoch 125: backward done\n",
      "epoch 125: update done\n",
      "Loss at 125: 0.033561\n",
      "epoch 125 test accuracy: 0.9893333333333333\n",
      "epoch 126: pred done\n",
      "epoch 126: backward done\n",
      "epoch 126: update done\n",
      "Loss at 126: 0.033472\n",
      "epoch 126 test accuracy: 0.9893333333333333\n",
      "epoch 127: pred done\n",
      "epoch 127: backward done\n",
      "epoch 127: update done\n",
      "Loss at 127: 0.033383\n",
      "epoch 127 test accuracy: 0.9893333333333333\n",
      "epoch 128: pred done\n",
      "epoch 128: backward done\n",
      "epoch 128: update done\n",
      "Loss at 128: 0.033296\n",
      "epoch 128 test accuracy: 0.9893333333333333\n",
      "epoch 129: pred done\n",
      "epoch 129: backward done\n",
      "epoch 129: update done\n",
      "Loss at 129: 0.033209\n",
      "epoch 129 test accuracy: 0.9893333333333333\n",
      "epoch 130: pred done\n",
      "epoch 130: backward done\n",
      "epoch 130: update done\n",
      "Loss at 130: 0.033123\n",
      "epoch 130 test accuracy: 0.9893333333333333\n",
      "epoch 131: pred done\n",
      "epoch 131: backward done\n",
      "epoch 131: update done\n",
      "Loss at 131: 0.033038\n",
      "epoch 131 test accuracy: 0.9893333333333333\n",
      "epoch 132: pred done\n",
      "epoch 132: backward done\n",
      "epoch 132: update done\n",
      "Loss at 132: 0.032954\n",
      "epoch 132 test accuracy: 0.9893333333333333\n",
      "epoch 133: pred done\n",
      "epoch 133: backward done\n",
      "epoch 133: update done\n",
      "Loss at 133: 0.032871\n",
      "epoch 133 test accuracy: 0.9893333333333333\n",
      "epoch 134: pred done\n",
      "epoch 134: backward done\n",
      "epoch 134: update done\n",
      "Loss at 134: 0.032789\n",
      "epoch 134 test accuracy: 0.9893333333333333\n",
      "epoch 135: pred done\n",
      "epoch 135: backward done\n",
      "epoch 135: update done\n",
      "Loss at 135: 0.032708\n",
      "epoch 135 test accuracy: 0.9893333333333333\n",
      "epoch 136: pred done\n",
      "epoch 136: backward done\n",
      "epoch 136: update done\n",
      "Loss at 136: 0.032627\n",
      "epoch 136 test accuracy: 0.9893333333333333\n",
      "epoch 137: pred done\n",
      "epoch 137: backward done\n",
      "epoch 137: update done\n",
      "Loss at 137: 0.032547\n",
      "epoch 137 test accuracy: 0.9893333333333333\n",
      "epoch 138: pred done\n",
      "epoch 138: backward done\n",
      "epoch 138: update done\n",
      "Loss at 138: 0.032468\n",
      "epoch 138 test accuracy: 0.9896666666666667\n",
      "epoch 139: pred done\n",
      "epoch 139: backward done\n",
      "epoch 139: update done\n",
      "Loss at 139: 0.032390\n",
      "epoch 139 test accuracy: 0.9896666666666667\n",
      "epoch 140: pred done\n",
      "epoch 140: backward done\n",
      "epoch 140: update done\n",
      "Loss at 140: 0.032312\n",
      "epoch 140 test accuracy: 0.9896666666666667\n",
      "epoch 141: pred done\n",
      "epoch 141: backward done\n",
      "epoch 141: update done\n",
      "Loss at 141: 0.032234\n",
      "epoch 141 test accuracy: 0.9896666666666667\n",
      "epoch 142: pred done\n",
      "epoch 142: backward done\n",
      "epoch 142: update done\n",
      "Loss at 142: 0.032157\n",
      "epoch 142 test accuracy: 0.9896666666666667\n",
      "epoch 143: pred done\n",
      "epoch 143: backward done\n",
      "epoch 143: update done\n",
      "Loss at 143: 0.032080\n",
      "epoch 143 test accuracy: 0.9896666666666667\n",
      "epoch 144: pred done\n",
      "epoch 144: backward done\n",
      "epoch 144: update done\n",
      "Loss at 144: 0.032005\n",
      "epoch 144 test accuracy: 0.9896666666666667\n",
      "epoch 145: pred done\n",
      "epoch 145: backward done\n",
      "epoch 145: update done\n",
      "Loss at 145: 0.031930\n",
      "epoch 145 test accuracy: 0.9896666666666667\n",
      "epoch 146: pred done\n",
      "epoch 146: backward done\n",
      "epoch 146: update done\n",
      "Loss at 146: 0.031855\n",
      "epoch 146 test accuracy: 0.9896666666666667\n",
      "epoch 147: pred done\n",
      "epoch 147: backward done\n",
      "epoch 147: update done\n",
      "Loss at 147: 0.031782\n",
      "epoch 147 test accuracy: 0.9896666666666667\n",
      "epoch 148: pred done\n",
      "epoch 148: backward done\n",
      "epoch 148: update done\n",
      "Loss at 148: 0.031709\n",
      "epoch 148 test accuracy: 0.9896666666666667\n",
      "epoch 149: pred done\n",
      "epoch 149: backward done\n",
      "epoch 149: update done\n",
      "Loss at 149: 0.031637\n",
      "epoch 149 test accuracy: 0.9896666666666667\n",
      "epoch 150: pred done\n",
      "epoch 150: backward done\n",
      "epoch 150: update done\n",
      "Loss at 150: 0.031565\n",
      "epoch 150 test accuracy: 0.9896666666666667\n",
      "epoch 151: pred done\n",
      "epoch 151: backward done\n",
      "epoch 151: update done\n",
      "Loss at 151: 0.031495\n",
      "epoch 151 test accuracy: 0.9896666666666667\n",
      "epoch 152: pred done\n",
      "epoch 152: backward done\n",
      "epoch 152: update done\n",
      "Loss at 152: 0.031424\n",
      "epoch 152 test accuracy: 0.9896666666666667\n",
      "epoch 153: pred done\n",
      "epoch 153: backward done\n",
      "epoch 153: update done\n",
      "Loss at 153: 0.031354\n",
      "epoch 153 test accuracy: 0.9896666666666667\n",
      "epoch 154: pred done\n",
      "epoch 154: backward done\n",
      "epoch 154: update done\n",
      "Loss at 154: 0.031285\n",
      "epoch 154 test accuracy: 0.9896666666666667\n",
      "epoch 155: pred done\n",
      "epoch 155: backward done\n",
      "epoch 155: update done\n",
      "Loss at 155: 0.031217\n",
      "epoch 155 test accuracy: 0.9896666666666667\n",
      "epoch 156: pred done\n",
      "epoch 156: backward done\n",
      "epoch 156: update done\n",
      "Loss at 156: 0.031148\n",
      "epoch 156 test accuracy: 0.9896666666666667\n",
      "epoch 157: pred done\n",
      "epoch 157: backward done\n",
      "epoch 157: update done\n",
      "Loss at 157: 0.031081\n",
      "epoch 157 test accuracy: 0.9896666666666667\n",
      "epoch 158: pred done\n",
      "epoch 158: backward done\n",
      "epoch 158: update done\n",
      "Loss at 158: 0.031014\n",
      "epoch 158 test accuracy: 0.9896666666666667\n",
      "epoch 159: pred done\n",
      "epoch 159: backward done\n",
      "epoch 159: update done\n",
      "Loss at 159: 0.030948\n",
      "epoch 159 test accuracy: 0.9896666666666667\n",
      "epoch 160: pred done\n",
      "epoch 160: backward done\n",
      "epoch 160: update done\n",
      "Loss at 160: 0.030882\n",
      "epoch 160 test accuracy: 0.9896666666666667\n",
      "epoch 161: pred done\n",
      "epoch 161: backward done\n",
      "epoch 161: update done\n",
      "Loss at 161: 0.030817\n",
      "epoch 161 test accuracy: 0.9896666666666667\n",
      "epoch 162: pred done\n",
      "epoch 162: backward done\n",
      "epoch 162: update done\n",
      "Loss at 162: 0.030752\n",
      "epoch 162 test accuracy: 0.9896666666666667\n",
      "epoch 163: pred done\n",
      "epoch 163: backward done\n",
      "epoch 163: update done\n",
      "Loss at 163: 0.030687\n",
      "epoch 163 test accuracy: 0.9896666666666667\n",
      "epoch 164: pred done\n",
      "epoch 164: backward done\n",
      "epoch 164: update done\n",
      "Loss at 164: 0.030623\n",
      "epoch 164 test accuracy: 0.9896666666666667\n",
      "epoch 165: pred done\n",
      "epoch 165: backward done\n",
      "epoch 165: update done\n",
      "Loss at 165: 0.030560\n",
      "epoch 165 test accuracy: 0.9896666666666667\n",
      "epoch 166: pred done\n",
      "epoch 166: backward done\n",
      "epoch 166: update done\n",
      "Loss at 166: 0.030496\n",
      "epoch 166 test accuracy: 0.9896666666666667\n",
      "epoch 167: pred done\n",
      "epoch 167: backward done\n",
      "epoch 167: update done\n",
      "Loss at 167: 0.030434\n",
      "epoch 167 test accuracy: 0.9896666666666667\n",
      "epoch 168: pred done\n",
      "epoch 168: backward done\n",
      "epoch 168: update done\n",
      "Loss at 168: 0.030372\n",
      "epoch 168 test accuracy: 0.99\n",
      "epoch 169: pred done\n",
      "epoch 169: backward done\n",
      "epoch 169: update done\n",
      "Loss at 169: 0.030310\n",
      "epoch 169 test accuracy: 0.99\n",
      "epoch 170: pred done\n",
      "epoch 170: backward done\n",
      "epoch 170: update done\n",
      "Loss at 170: 0.030249\n",
      "epoch 170 test accuracy: 0.99\n",
      "epoch 171: pred done\n",
      "epoch 171: backward done\n",
      "epoch 171: update done\n",
      "Loss at 171: 0.030188\n",
      "epoch 171 test accuracy: 0.99\n",
      "epoch 172: pred done\n",
      "epoch 172: backward done\n",
      "epoch 172: update done\n",
      "Loss at 172: 0.030127\n",
      "epoch 172 test accuracy: 0.99\n",
      "epoch 173: pred done\n",
      "epoch 173: backward done\n",
      "epoch 173: update done\n",
      "Loss at 173: 0.030067\n",
      "epoch 173 test accuracy: 0.99\n",
      "epoch 174: pred done\n",
      "epoch 174: backward done\n",
      "epoch 174: update done\n",
      "Loss at 174: 0.030008\n",
      "epoch 174 test accuracy: 0.99\n",
      "epoch 175: pred done\n",
      "epoch 175: backward done\n",
      "epoch 175: update done\n",
      "Loss at 175: 0.029948\n",
      "epoch 175 test accuracy: 0.99\n",
      "epoch 176: pred done\n",
      "epoch 176: backward done\n",
      "epoch 176: update done\n",
      "Loss at 176: 0.029889\n",
      "epoch 176 test accuracy: 0.99\n",
      "epoch 177: pred done\n",
      "epoch 177: backward done\n",
      "epoch 177: update done\n",
      "Loss at 177: 0.029831\n",
      "epoch 177 test accuracy: 0.99\n",
      "epoch 178: pred done\n",
      "epoch 178: backward done\n",
      "epoch 178: update done\n",
      "Loss at 178: 0.029773\n",
      "epoch 178 test accuracy: 0.99\n",
      "epoch 179: pred done\n",
      "epoch 179: backward done\n",
      "epoch 179: update done\n",
      "Loss at 179: 0.029715\n",
      "epoch 179 test accuracy: 0.99\n",
      "epoch 180: pred done\n",
      "epoch 180: backward done\n",
      "epoch 180: update done\n",
      "Loss at 180: 0.029657\n",
      "epoch 180 test accuracy: 0.99\n",
      "epoch 181: pred done\n",
      "epoch 181: backward done\n",
      "epoch 181: update done\n",
      "Loss at 181: 0.029600\n",
      "epoch 181 test accuracy: 0.99\n",
      "epoch 182: pred done\n",
      "epoch 182: backward done\n",
      "epoch 182: update done\n",
      "Loss at 182: 0.029544\n",
      "epoch 182 test accuracy: 0.99\n",
      "epoch 183: pred done\n",
      "epoch 183: backward done\n",
      "epoch 183: update done\n",
      "Loss at 183: 0.029487\n",
      "epoch 183 test accuracy: 0.99\n",
      "epoch 184: pred done\n",
      "epoch 184: backward done\n",
      "epoch 184: update done\n",
      "Loss at 184: 0.029432\n",
      "epoch 184 test accuracy: 0.99\n",
      "epoch 185: pred done\n",
      "epoch 185: backward done\n",
      "epoch 185: update done\n",
      "Loss at 185: 0.029376\n",
      "epoch 185 test accuracy: 0.99\n",
      "epoch 186: pred done\n",
      "epoch 186: backward done\n",
      "epoch 186: update done\n",
      "Loss at 186: 0.029321\n",
      "epoch 186 test accuracy: 0.99\n",
      "epoch 187: pred done\n",
      "epoch 187: backward done\n",
      "epoch 187: update done\n",
      "Loss at 187: 0.029266\n",
      "epoch 187 test accuracy: 0.99\n",
      "epoch 188: pred done\n",
      "epoch 188: backward done\n",
      "epoch 188: update done\n",
      "Loss at 188: 0.029212\n",
      "epoch 188 test accuracy: 0.99\n",
      "epoch 189: pred done\n",
      "epoch 189: backward done\n",
      "epoch 189: update done\n",
      "Loss at 189: 0.029157\n",
      "epoch 189 test accuracy: 0.99\n",
      "epoch 190: pred done\n",
      "epoch 190: backward done\n",
      "epoch 190: update done\n",
      "Loss at 190: 0.029104\n",
      "epoch 190 test accuracy: 0.99\n",
      "epoch 191: pred done\n",
      "epoch 191: backward done\n",
      "epoch 191: update done\n",
      "Loss at 191: 0.029050\n",
      "epoch 191 test accuracy: 0.99\n",
      "epoch 192: pred done\n",
      "epoch 192: backward done\n",
      "epoch 192: update done\n",
      "Loss at 192: 0.028997\n",
      "epoch 192 test accuracy: 0.99\n",
      "epoch 193: pred done\n",
      "epoch 193: backward done\n",
      "epoch 193: update done\n",
      "Loss at 193: 0.028944\n",
      "epoch 193 test accuracy: 0.99\n",
      "epoch 194: pred done\n",
      "epoch 194: backward done\n",
      "epoch 194: update done\n",
      "Loss at 194: 0.028891\n",
      "epoch 194 test accuracy: 0.9903333333333333\n",
      "epoch 195: pred done\n",
      "epoch 195: backward done\n",
      "epoch 195: update done\n",
      "Loss at 195: 0.028839\n",
      "epoch 195 test accuracy: 0.9903333333333333\n",
      "epoch 196: pred done\n",
      "epoch 196: backward done\n",
      "epoch 196: update done\n",
      "Loss at 196: 0.028787\n",
      "epoch 196 test accuracy: 0.9903333333333333\n",
      "epoch 197: pred done\n",
      "epoch 197: backward done\n",
      "epoch 197: update done\n",
      "Loss at 197: 0.028735\n",
      "epoch 197 test accuracy: 0.9903333333333333\n",
      "epoch 198: pred done\n",
      "epoch 198: backward done\n",
      "epoch 198: update done\n",
      "Loss at 198: 0.028683\n",
      "epoch 198 test accuracy: 0.9903333333333333\n",
      "epoch 199: pred done\n",
      "epoch 199: backward done\n",
      "epoch 199: update done\n",
      "Loss at 199: 0.028631\n",
      "epoch 199 test accuracy: 0.9903333333333333\n",
      "epoch 200: pred done\n",
      "epoch 200: backward done\n",
      "epoch 200: update done\n",
      "Loss at 200: 0.028580\n",
      "epoch 200 test accuracy: 0.9903333333333333\n",
      "epoch 201: pred done\n",
      "epoch 201: backward done\n",
      "epoch 201: update done\n",
      "Loss at 201: 0.028529\n",
      "epoch 201 test accuracy: 0.9903333333333333\n",
      "epoch 202: pred done\n",
      "epoch 202: backward done\n",
      "epoch 202: update done\n",
      "Loss at 202: 0.028477\n",
      "epoch 202 test accuracy: 0.9903333333333333\n",
      "epoch 203: pred done\n",
      "epoch 203: backward done\n",
      "epoch 203: update done\n",
      "Loss at 203: 0.028427\n",
      "epoch 203 test accuracy: 0.9903333333333333\n",
      "epoch 204: pred done\n",
      "epoch 204: backward done\n",
      "epoch 204: update done\n",
      "Loss at 204: 0.028376\n",
      "epoch 204 test accuracy: 0.9903333333333333\n",
      "epoch 205: pred done\n",
      "epoch 205: backward done\n",
      "epoch 205: update done\n",
      "Loss at 205: 0.028326\n",
      "epoch 205 test accuracy: 0.9903333333333333\n",
      "epoch 206: pred done\n",
      "epoch 206: backward done\n",
      "epoch 206: update done\n",
      "Loss at 206: 0.028275\n",
      "epoch 206 test accuracy: 0.9903333333333333\n",
      "epoch 207: pred done\n",
      "epoch 207: backward done\n",
      "epoch 207: update done\n",
      "Loss at 207: 0.028225\n",
      "epoch 207 test accuracy: 0.9903333333333333\n",
      "epoch 208: pred done\n",
      "epoch 208: backward done\n",
      "epoch 208: update done\n",
      "Loss at 208: 0.028176\n",
      "epoch 208 test accuracy: 0.9903333333333333\n",
      "epoch 209: pred done\n",
      "epoch 209: backward done\n",
      "epoch 209: update done\n",
      "Loss at 209: 0.028126\n",
      "epoch 209 test accuracy: 0.9903333333333333\n",
      "epoch 210: pred done\n",
      "epoch 210: backward done\n",
      "epoch 210: update done\n",
      "Loss at 210: 0.028077\n",
      "epoch 210 test accuracy: 0.9903333333333333\n",
      "epoch 211: pred done\n",
      "epoch 211: backward done\n",
      "epoch 211: update done\n",
      "Loss at 211: 0.028027\n",
      "epoch 211 test accuracy: 0.9903333333333333\n",
      "epoch 212: pred done\n",
      "epoch 212: backward done\n",
      "epoch 212: update done\n",
      "Loss at 212: 0.027978\n",
      "epoch 212 test accuracy: 0.9903333333333333\n",
      "epoch 213: pred done\n",
      "epoch 213: backward done\n",
      "epoch 213: update done\n",
      "Loss at 213: 0.027929\n",
      "epoch 213 test accuracy: 0.9903333333333333\n",
      "epoch 214: pred done\n",
      "epoch 214: backward done\n",
      "epoch 214: update done\n",
      "Loss at 214: 0.027880\n",
      "epoch 214 test accuracy: 0.9903333333333333\n",
      "epoch 215: pred done\n",
      "epoch 215: backward done\n",
      "epoch 215: update done\n",
      "Loss at 215: 0.027832\n",
      "epoch 215 test accuracy: 0.9903333333333333\n",
      "epoch 216: pred done\n",
      "epoch 216: backward done\n",
      "epoch 216: update done\n",
      "Loss at 216: 0.027784\n",
      "epoch 216 test accuracy: 0.9903333333333333\n",
      "epoch 217: pred done\n",
      "epoch 217: backward done\n",
      "epoch 217: update done\n",
      "Loss at 217: 0.027736\n",
      "epoch 217 test accuracy: 0.9903333333333333\n",
      "epoch 218: pred done\n",
      "epoch 218: backward done\n",
      "epoch 218: update done\n",
      "Loss at 218: 0.027689\n",
      "epoch 218 test accuracy: 0.9903333333333333\n",
      "epoch 219: pred done\n",
      "epoch 219: backward done\n",
      "epoch 219: update done\n",
      "Loss at 219: 0.027642\n",
      "epoch 219 test accuracy: 0.9903333333333333\n",
      "epoch 220: pred done\n",
      "epoch 220: backward done\n",
      "epoch 220: update done\n",
      "Loss at 220: 0.027595\n",
      "epoch 220 test accuracy: 0.9903333333333333\n",
      "epoch 221: pred done\n",
      "epoch 221: backward done\n",
      "epoch 221: update done\n",
      "Loss at 221: 0.027548\n",
      "epoch 221 test accuracy: 0.9903333333333333\n",
      "epoch 222: pred done\n",
      "epoch 222: backward done\n",
      "epoch 222: update done\n",
      "Loss at 222: 0.027502\n",
      "epoch 222 test accuracy: 0.9903333333333333\n",
      "epoch 223: pred done\n",
      "epoch 223: backward done\n",
      "epoch 223: update done\n",
      "Loss at 223: 0.027455\n",
      "epoch 223 test accuracy: 0.9903333333333333\n",
      "epoch 224: pred done\n",
      "epoch 224: backward done\n",
      "epoch 224: update done\n",
      "Loss at 224: 0.027410\n",
      "epoch 224 test accuracy: 0.9903333333333333\n",
      "epoch 225: pred done\n",
      "epoch 225: backward done\n",
      "epoch 225: update done\n",
      "Loss at 225: 0.027364\n",
      "epoch 225 test accuracy: 0.9903333333333333\n",
      "epoch 226: pred done\n",
      "epoch 226: backward done\n",
      "epoch 226: update done\n",
      "Loss at 226: 0.027319\n",
      "epoch 226 test accuracy: 0.9903333333333333\n",
      "epoch 227: pred done\n",
      "epoch 227: backward done\n",
      "epoch 227: update done\n",
      "Loss at 227: 0.027274\n",
      "epoch 227 test accuracy: 0.9903333333333333\n",
      "epoch 228: pred done\n",
      "epoch 228: backward done\n",
      "epoch 228: update done\n",
      "Loss at 228: 0.027229\n",
      "epoch 228 test accuracy: 0.9903333333333333\n",
      "epoch 229: pred done\n",
      "epoch 229: backward done\n",
      "epoch 229: update done\n",
      "Loss at 229: 0.027185\n",
      "epoch 229 test accuracy: 0.9903333333333333\n",
      "epoch 230: pred done\n",
      "epoch 230: backward done\n",
      "epoch 230: update done\n",
      "Loss at 230: 0.027140\n",
      "epoch 230 test accuracy: 0.9903333333333333\n",
      "epoch 231: pred done\n",
      "epoch 231: backward done\n",
      "epoch 231: update done\n",
      "Loss at 231: 0.027096\n",
      "epoch 231 test accuracy: 0.9903333333333333\n",
      "epoch 232: pred done\n",
      "epoch 232: backward done\n",
      "epoch 232: update done\n",
      "Loss at 232: 0.027052\n",
      "epoch 232 test accuracy: 0.9903333333333333\n",
      "epoch 233: pred done\n",
      "epoch 233: backward done\n",
      "epoch 233: update done\n",
      "Loss at 233: 0.027008\n",
      "epoch 233 test accuracy: 0.9903333333333333\n",
      "epoch 234: pred done\n",
      "epoch 234: backward done\n",
      "epoch 234: update done\n",
      "Loss at 234: 0.026964\n",
      "epoch 234 test accuracy: 0.9903333333333333\n",
      "epoch 235: pred done\n",
      "epoch 235: backward done\n",
      "epoch 235: update done\n",
      "Loss at 235: 0.026921\n",
      "epoch 235 test accuracy: 0.9903333333333333\n",
      "epoch 236: pred done\n",
      "epoch 236: backward done\n",
      "epoch 236: update done\n",
      "Loss at 236: 0.026878\n",
      "epoch 236 test accuracy: 0.9903333333333333\n",
      "epoch 237: pred done\n",
      "epoch 237: backward done\n",
      "epoch 237: update done\n",
      "Loss at 237: 0.026834\n",
      "epoch 237 test accuracy: 0.9903333333333333\n",
      "epoch 238: pred done\n",
      "epoch 238: backward done\n",
      "epoch 238: update done\n",
      "Loss at 238: 0.026792\n",
      "epoch 238 test accuracy: 0.9903333333333333\n",
      "epoch 239: pred done\n",
      "epoch 239: backward done\n",
      "epoch 239: update done\n",
      "Loss at 239: 0.026749\n",
      "epoch 239 test accuracy: 0.9903333333333333\n",
      "epoch 240: pred done\n",
      "epoch 240: backward done\n",
      "epoch 240: update done\n",
      "Loss at 240: 0.026706\n",
      "epoch 240 test accuracy: 0.9903333333333333\n",
      "epoch 241: pred done\n",
      "epoch 241: backward done\n",
      "epoch 241: update done\n",
      "Loss at 241: 0.026663\n",
      "epoch 241 test accuracy: 0.9903333333333333\n",
      "epoch 242: pred done\n",
      "epoch 242: backward done\n",
      "epoch 242: update done\n",
      "Loss at 242: 0.026621\n",
      "epoch 242 test accuracy: 0.9906666666666667\n",
      "epoch 243: pred done\n",
      "epoch 243: backward done\n",
      "epoch 243: update done\n",
      "Loss at 243: 0.026579\n",
      "epoch 243 test accuracy: 0.9906666666666667\n",
      "epoch 244: pred done\n",
      "epoch 244: backward done\n",
      "epoch 244: update done\n",
      "Loss at 244: 0.026537\n",
      "epoch 244 test accuracy: 0.9906666666666667\n",
      "epoch 245: pred done\n",
      "epoch 245: backward done\n",
      "epoch 245: update done\n",
      "Loss at 245: 0.026495\n",
      "epoch 245 test accuracy: 0.9906666666666667\n",
      "epoch 246: pred done\n",
      "epoch 246: backward done\n",
      "epoch 246: update done\n",
      "Loss at 246: 0.026454\n",
      "epoch 246 test accuracy: 0.9906666666666667\n",
      "epoch 247: pred done\n",
      "epoch 247: backward done\n",
      "epoch 247: update done\n",
      "Loss at 247: 0.026412\n",
      "epoch 247 test accuracy: 0.9906666666666667\n",
      "epoch 248: pred done\n",
      "epoch 248: backward done\n",
      "epoch 248: update done\n",
      "Loss at 248: 0.026371\n",
      "epoch 248 test accuracy: 0.9906666666666667\n",
      "epoch 249: pred done\n",
      "epoch 249: backward done\n",
      "epoch 249: update done\n",
      "Loss at 249: 0.026330\n",
      "epoch 249 test accuracy: 0.9906666666666667\n",
      "epoch 250: pred done\n",
      "epoch 250: backward done\n",
      "epoch 250: update done\n",
      "Loss at 250: 0.026289\n",
      "epoch 250 test accuracy: 0.9906666666666667\n",
      "epoch 251: pred done\n",
      "epoch 251: backward done\n",
      "epoch 251: update done\n",
      "Loss at 251: 0.026248\n",
      "epoch 251 test accuracy: 0.9906666666666667\n",
      "epoch 252: pred done\n",
      "epoch 252: backward done\n",
      "epoch 252: update done\n",
      "Loss at 252: 0.026208\n",
      "epoch 252 test accuracy: 0.9906666666666667\n",
      "epoch 253: pred done\n",
      "epoch 253: backward done\n",
      "epoch 253: update done\n",
      "Loss at 253: 0.026167\n",
      "epoch 253 test accuracy: 0.9906666666666667\n",
      "epoch 254: pred done\n",
      "epoch 254: backward done\n",
      "epoch 254: update done\n",
      "Loss at 254: 0.026127\n",
      "epoch 254 test accuracy: 0.9906666666666667\n",
      "epoch 255: pred done\n",
      "epoch 255: backward done\n",
      "epoch 255: update done\n",
      "Loss at 255: 0.026087\n",
      "epoch 255 test accuracy: 0.9906666666666667\n",
      "epoch 256: pred done\n",
      "epoch 256: backward done\n",
      "epoch 256: update done\n",
      "Loss at 256: 0.026047\n",
      "epoch 256 test accuracy: 0.9906666666666667\n",
      "epoch 257: pred done\n",
      "epoch 257: backward done\n",
      "epoch 257: update done\n",
      "Loss at 257: 0.026007\n",
      "epoch 257 test accuracy: 0.9906666666666667\n",
      "epoch 258: pred done\n",
      "epoch 258: backward done\n",
      "epoch 258: update done\n",
      "Loss at 258: 0.025967\n",
      "epoch 258 test accuracy: 0.9906666666666667\n",
      "epoch 259: pred done\n",
      "epoch 259: backward done\n",
      "epoch 259: update done\n",
      "Loss at 259: 0.025928\n",
      "epoch 259 test accuracy: 0.9906666666666667\n",
      "epoch 260: pred done\n",
      "epoch 260: backward done\n",
      "epoch 260: update done\n",
      "Loss at 260: 0.025888\n",
      "epoch 260 test accuracy: 0.9906666666666667\n",
      "epoch 261: pred done\n",
      "epoch 261: backward done\n",
      "epoch 261: update done\n",
      "Loss at 261: 0.025849\n",
      "epoch 261 test accuracy: 0.9906666666666667\n",
      "epoch 262: pred done\n",
      "epoch 262: backward done\n",
      "epoch 262: update done\n",
      "Loss at 262: 0.025810\n",
      "epoch 262 test accuracy: 0.9906666666666667\n",
      "epoch 263: pred done\n",
      "epoch 263: backward done\n",
      "epoch 263: update done\n",
      "Loss at 263: 0.025771\n",
      "epoch 263 test accuracy: 0.9906666666666667\n",
      "epoch 264: pred done\n",
      "epoch 264: backward done\n",
      "epoch 264: update done\n",
      "Loss at 264: 0.025732\n",
      "epoch 264 test accuracy: 0.9906666666666667\n",
      "epoch 265: pred done\n",
      "epoch 265: backward done\n",
      "epoch 265: update done\n",
      "Loss at 265: 0.025693\n",
      "epoch 265 test accuracy: 0.9906666666666667\n",
      "epoch 266: pred done\n",
      "epoch 266: backward done\n",
      "epoch 266: update done\n",
      "Loss at 266: 0.025654\n",
      "epoch 266 test accuracy: 0.9906666666666667\n",
      "epoch 267: pred done\n",
      "epoch 267: backward done\n",
      "epoch 267: update done\n",
      "Loss at 267: 0.025615\n",
      "epoch 267 test accuracy: 0.9906666666666667\n",
      "epoch 268: pred done\n",
      "epoch 268: backward done\n",
      "epoch 268: update done\n",
      "Loss at 268: 0.025577\n",
      "epoch 268 test accuracy: 0.9906666666666667\n",
      "epoch 269: pred done\n",
      "epoch 269: backward done\n",
      "epoch 269: update done\n",
      "Loss at 269: 0.025539\n",
      "epoch 269 test accuracy: 0.9906666666666667\n",
      "epoch 270: pred done\n",
      "epoch 270: backward done\n",
      "epoch 270: update done\n",
      "Loss at 270: 0.025500\n",
      "epoch 270 test accuracy: 0.9906666666666667\n",
      "epoch 271: pred done\n",
      "epoch 271: backward done\n",
      "epoch 271: update done\n",
      "Loss at 271: 0.025462\n",
      "epoch 271 test accuracy: 0.9906666666666667\n",
      "epoch 272: pred done\n",
      "epoch 272: backward done\n",
      "epoch 272: update done\n",
      "Loss at 272: 0.025425\n",
      "epoch 272 test accuracy: 0.9906666666666667\n",
      "epoch 273: pred done\n",
      "epoch 273: backward done\n",
      "epoch 273: update done\n",
      "Loss at 273: 0.025387\n",
      "epoch 273 test accuracy: 0.9906666666666667\n",
      "epoch 274: pred done\n",
      "epoch 274: backward done\n",
      "epoch 274: update done\n",
      "Loss at 274: 0.025349\n",
      "epoch 274 test accuracy: 0.9906666666666667\n",
      "epoch 275: pred done\n",
      "epoch 275: backward done\n",
      "epoch 275: update done\n",
      "Loss at 275: 0.025312\n",
      "epoch 275 test accuracy: 0.9906666666666667\n",
      "epoch 276: pred done\n",
      "epoch 276: backward done\n",
      "epoch 276: update done\n",
      "Loss at 276: 0.025275\n",
      "epoch 276 test accuracy: 0.9906666666666667\n",
      "epoch 277: pred done\n",
      "epoch 277: backward done\n",
      "epoch 277: update done\n",
      "Loss at 277: 0.025238\n",
      "epoch 277 test accuracy: 0.9906666666666667\n",
      "epoch 278: pred done\n",
      "epoch 278: backward done\n",
      "epoch 278: update done\n",
      "Loss at 278: 0.025201\n",
      "epoch 278 test accuracy: 0.9906666666666667\n",
      "epoch 279: pred done\n",
      "epoch 279: backward done\n",
      "epoch 279: update done\n",
      "Loss at 279: 0.025165\n",
      "epoch 279 test accuracy: 0.9906666666666667\n",
      "epoch 280: pred done\n",
      "epoch 280: backward done\n",
      "epoch 280: update done\n",
      "Loss at 280: 0.025128\n",
      "epoch 280 test accuracy: 0.9906666666666667\n",
      "epoch 281: pred done\n",
      "epoch 281: backward done\n",
      "epoch 281: update done\n",
      "Loss at 281: 0.025092\n",
      "epoch 281 test accuracy: 0.9906666666666667\n",
      "epoch 282: pred done\n",
      "epoch 282: backward done\n",
      "epoch 282: update done\n",
      "Loss at 282: 0.025056\n",
      "epoch 282 test accuracy: 0.9906666666666667\n",
      "epoch 283: pred done\n",
      "epoch 283: backward done\n",
      "epoch 283: update done\n",
      "Loss at 283: 0.025019\n",
      "epoch 283 test accuracy: 0.9906666666666667\n",
      "epoch 284: pred done\n",
      "epoch 284: backward done\n",
      "epoch 284: update done\n",
      "Loss at 284: 0.024984\n",
      "epoch 284 test accuracy: 0.9906666666666667\n",
      "epoch 285: pred done\n",
      "epoch 285: backward done\n",
      "epoch 285: update done\n",
      "Loss at 285: 0.024948\n",
      "epoch 285 test accuracy: 0.9906666666666667\n",
      "epoch 286: pred done\n",
      "epoch 286: backward done\n",
      "epoch 286: update done\n",
      "Loss at 286: 0.024912\n",
      "epoch 286 test accuracy: 0.9906666666666667\n",
      "epoch 287: pred done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Train the network using only `x_train` and `y_train` (no validation)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train(model, epochs, x_train, y_train)\n",
      "Cell \u001b[0;32mIn[121], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, x, y, start_epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m: pred done\u001b[39m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(pred, y)\n\u001b[0;32m---> 10\u001b[0m model\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m: backward done\u001b[39m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mupdate()\n",
      "Cell \u001b[0;32mIn[97], line 41\u001b[0m, in \u001b[0;36mMLP.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(layer, \u001b[39m'\u001b[39m\u001b[39mbackward_input\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m         layer\u001b[39m.\u001b[39;49mbackward(grad)\n\u001b[1;32m     42\u001b[0m         grad \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mbackward_input(grad)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[96], line 37\u001b[0m, in \u001b[0;36mLinear.backward\u001b[0;34m(self, up_grad)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, up_grad):\n\u001b[1;32m     30\u001b[0m     \u001b[39m# Calculate the gradient with respect to the weights \u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[39m# and biases and save the results.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39m#############################\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Your code goes here (6 points)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39m#: up_grad: (B, F_out)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minp\n\u001b[0;32m---> 37\u001b[0m     down_grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39m...i,...j->...ij\u001b[39;49m\u001b[39m'\u001b[39;49m, inp, up_grad) \u001b[39m#: (B, F_in, F_out)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     down_grad_b \u001b[39m=\u001b[39m up_grad \u001b[39m#: (B, F_out)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdw \u001b[39m=\u001b[39m down_grad\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/core/einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[39mif\u001b[39;00m specified_out:\n\u001b[1;32m   1370\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m out\n\u001b[0;32m-> 1371\u001b[0m     \u001b[39mreturn\u001b[39;00m c_einsum(\u001b[39m*\u001b[39;49moperands, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1373\u001b[0m \u001b[39m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[39m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m valid_einsum_kwargs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcasting\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the `MLP` with the following structure:\n",
    "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
    "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "model = MLP([Linear(784, 50, seed=43), ReLU(), Linear(50, 50, seed=44), ReLU(), Linear(50, 3, seed=45),\n",
    "            #  Sigmoid(), \n",
    "             SoftMaxLayer()], CELoss(), \n",
    "            #  GradientDescent(ic(0.3*(y_train.shape[0]))),\n",
    "\t\t\t GradientDescent(0.1),\n",
    "            #  GradientDescent(0.01),\n",
    "            #  GradientDescent(0.001),\n",
    "             )\n",
    "#############################\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# Train the network using only `x_train` and `y_train` (no validation)\n",
    "train(model, epochs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJec2xRJmY37"
   },
   "source": [
    "Let's plot the loss value for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model2.losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "ymaQNn70cdnZ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz3yqRa1cdna"
   },
   "source": [
    "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 784)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "TRqwXho7cdnd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9906666666666667\n",
      "0.9923333333333333\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the testing set\n",
    "#############################\n",
    "# Your code goes here (7 points)\n",
    "\n",
    "\n",
    "acc = accuracy(model, x_test, y_test)\n",
    "acc2 = accuracy(model2, x_test, y_test)\n",
    "#############################\n",
    "\n",
    "print(acc)\n",
    "print(acc2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
